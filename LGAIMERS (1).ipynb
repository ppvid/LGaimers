{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TVAQx5VKuhcM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tEbTprqhulbt"},"outputs":[],"source":["import os\n","import random\n","import glob\n","import re\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LhNo65hruqQk"},"outputs":[],"source":["def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"W1_0YDOCutB4"},"outputs":[],"source":["df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/LGaimers/á„’á…¢á„á…¥á„á…©á†«/train/train.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cWces1s_uv0I"},"outputs":[],"source":["# 'ì˜ì—…ì¥ëª…'ê³¼ 'ë©”ë‰´ëª…' ë¶„ë¦¬\n","df[['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…']] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_', n=1, expand=True)\n","df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])  # ë¬¸ìì—´ â†’ datetime ë³€í™˜"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QqaVTU-9u05r"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-01-01 00:00:00 2024-06-15 00:00:00\n"]}],"source":["print(df['ì˜ì—…ì¼ì'].min(), df['ì˜ì—…ì¼ì'].max())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zYJLzhtAwmg8"},"outputs":[],"source":["zero_sales_dict = {}\n","\n","# ë©”ë‰´ë³„ 0íŒë§¤ êµ¬ê°„ íƒì§€\n","for menu, group in df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'):\n","    group = group.sort_values('ì˜ì—…ì¼ì').reset_index(drop=True)\n","    zero_sales = group[group['ë§¤ì¶œìˆ˜ëŸ‰'] == 0].copy()\n","\n","    if zero_sales.empty:\n","        continue\n","\n","    zero_sales['ë‚ ì§œì°¨ì´'] = zero_sales['ì˜ì—…ì¼ì'].diff().dt.days.fillna(1)\n","    group_id = (zero_sales['ë‚ ì§œì°¨ì´'] != 1).cumsum()\n","\n","    result_list = []\n","    for _, sub in zero_sales.groupby(group_id):\n","        dates = sub['ì˜ì—…ì¼ì'].dt.date.tolist()\n","        if len(dates) == 1:\n","            result_list.append(f\"({dates[0]})\")\n","        else:\n","            result_list.append(f\"({dates[0]}, {dates[-1]})\")\n","    zero_sales_dict[menu] = result_list"]},{"cell_type":"markdown","metadata":{"id":"bRwBiH-sw73C"},"source":["is_zero_sales_periodê°€ 1ì´ë©´ ì¼ì£¼ì¼ ì´ìƒ íŒë§¤ ì¤‘ë‹¨ ê¸°ê°„"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1DNQyo_ww4uW"},"outputs":[],"source":["n = 7  # íŒë§¤ ì¤‘ë‹¨ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ì—°ì†ì¼ ìˆ˜\n","\n","df['is_zero_sales_period'] = 0  # ì´ˆê¸°í™”\n","\n","for menu, periods in zero_sales_dict.items():\n","    for period in periods:\n","        dates = period.strip(\"()\").split(\", \")\n","        if len(dates) == 1:\n","            continue  # ë‹¨ì¼ ë‚ ì§œëŠ” ì œì™¸\n","        else:\n","            d1 = pd.to_datetime(dates[0])\n","            d2 = pd.to_datetime(dates[1])\n","            duration = (d2 - d1).days + 1\n","\n","            if duration \u003e= n:\n","                df.loc[\n","                    (df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == menu) \u0026\n","                    (df['ì˜ì—…ì¼ì'].between(d1, d2)),\n","                    'is_zero_sales_period'\n","                ] = 1"]},{"cell_type":"markdown","metadata":{"id":"jhuG2nzxXzcW"},"source":["ì˜ì—…ì¥ë³„ 4ì¼ ì´ìƒ ë§¤ì¶œ ì—†ëŠ” ê¸°ê°„ ì •ê¸°ì  íœ´ë¬´ì¼ ì§€ì •"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IvEUYkVSWS4C"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def find_store_zero_runs(df: pd.DataFrame,\n","                         date_col: str = 'ì˜ì—…ì¼ì',\n","                         store_col: str = 'ì˜ì—…ì¥ëª…',\n","                         sales_col: str = 'ë§¤ì¶œìˆ˜ëŸ‰',\n","                         min_run: int = 7,\n","                         treat_missing_as_zero: bool = True) -\u003e pd.DataFrame:\n","    \"\"\"\n","    ì˜ì—…ì¥ë³„ë¡œ (ëª¨ë“  ë©”ë‰´ í•©ê³„ ê¸°ì¤€) ë§¤ì¶œìˆ˜ëŸ‰==0 ì´ min_runì¼ ì´ìƒ ì—°ì†ì¸ ê¸°ê°„ì„ ë°˜í™˜.\n","    ë°˜í™˜ ì»¬ëŸ¼: [ì˜ì—…ì¥ëª…, start, end, run_length]\n","\n","    treat_missing_as_zero:\n","      True  -\u003e ì›ë³¸ì— ì—†ëŠ” ë‚ ì§œ(ê²°ì¸¡ ì¼ì)ë¥¼ 0ìœ¼ë¡œ ê°„ì£¼(ë¦¬ìƒ˜í”Œë§ í›„ 0 ì±„ì›€)\n","      False -\u003e ì›ë³¸ì— ìˆëŠ” ë‚ ì§œë§Œ ì‚¬ìš©(ì—°ì†ì„±ì€ ì›ë³¸ ì¼ì ê¸°ì¤€)\n","    \"\"\"\n","    df = df.copy()\n","    df[date_col] = pd.to_datetime(df[date_col])\n","\n","    # 1) ì˜ì—…ì¥Ã—ë‚ ì§œë³„ ì´ ë§¤ì¶œ(ëª¨ë“  ë©”ë‰´ í•©ê³„)\n","    daily = (df.groupby([store_col, date_col], as_index=False)[sales_col]\n","               .sum()\n","               .rename(columns={sales_col: 'store_sales'}))\n","\n","    results = []\n","\n","    for store, g in daily.groupby(store_col, sort=False):\n","        g = g.sort_values(date_col)\n","\n","        if treat_missing_as_zero:\n","            # ì˜ì—…ì¥ë³„ ì „ì²´ ë‚ ì§œ ì¸ë±ìŠ¤ë¡œ ë¦¬ìƒ˜í”Œë§ â†’ ê²°ì¸¡ì¼ 0ìœ¼ë¡œ ì±„ì›€\n","            full_idx = pd.date_range(g[date_col].min(), g[date_col].max(), freq='D')\n","            s = (g.set_index(date_col)['store_sales']\n","                   .reindex(full_idx)\n","                   .fillna(0.0))\n","            s.index.name = date_col\n","        else:\n","            # ì›ë³¸ì— ì¡´ì¬í•˜ëŠ” ë‚ ì§œë§Œ ì‚¬ìš©(ë‚ ì§œ ê°„ê²©ì´ ë„ì—„ë„ì—„ì¼ ìˆ˜ ìˆìŒ)\n","            s = g.set_index(date_col)['store_sales'].copy()\n","\n","        # 2) 0/ë¹„0 í”Œë˜ê·¸\n","        z = (s == 0).astype(int)\n","\n","        if z.sum() == 0:\n","            continue  # 0ì´ í•œ ë²ˆë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ\n","\n","        # 3) ì—°ì† êµ¬ê°„(run) êµ¬ë¶„: ê°’ì´ ë°”ë€” ë•Œë§ˆë‹¤ run_id ì¦ê°€\n","        run_id = (z.diff().fillna(z.iloc[0]) != 0).cumsum()\n","\n","        # 4) runë³„ ìš”ì•½\n","        tmp = (\n","            pd.DataFrame({'flag_zero': z, 'run_id': run_id})\n","            .groupby('run_id', group_keys=False)\n","            .apply(lambda d: pd.Series({\n","                'flag_zero': d['flag_zero'].iloc[0],\n","                'start':     d.index.min(),\n","                'end':       d.index.max(),\n","                'run_length': len(d)\n","            }))\n","        )\n","\n","        # 5) 0ì¸ êµ¬ê°„ + ê¸¸ì´ í•„í„°\n","        zero_runs = tmp[(tmp['flag_zero'] == 1) \u0026 (tmp['run_length'] \u003e= min_run)]\n","        if zero_runs.empty:\n","            continue\n","\n","        # 6) ê²°ê³¼ ì •ë¦¬\n","        for _, row in zero_runs.iterrows():\n","            results.append({\n","                store_col: store,\n","                'start':   row['start'],\n","                'end':     row['end'],\n","                'run_length': int(row['run_length']),\n","            })\n","\n","    out = pd.DataFrame(results).sort_values([store_col, 'start']).reset_index(drop=True)\n","    return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CnWfxzbdWaxX"},"outputs":[],"source":["def print_zero_runs_by_store(zero_runs_df, store_col='ì˜ì—…ì¥ëª…'):\n","    \"\"\"\n","    find_store_zero_runs() ê²°ê³¼ë¥¼ ì˜ì—…ì¥ë³„ë¡œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜.\n","    \"\"\"\n","    stores = zero_runs_df[store_col].unique()\n","    for store in stores:\n","        print(f\"\\n=== {store} ===\")\n","        runs = zero_runs_df[zero_runs_df[store_col] == store]\n","        if runs.empty:\n","            print(\"  (ì—°ì† 0 êµ¬ê°„ ì—†ìŒ)\")\n","        else:\n","            for _, row in runs.iterrows():\n","                start_str = row['start'].strftime('%Y-%m-%d')\n","                end_str = row['end'].strftime('%Y-%m-%d')\n","                print(f\"  {start_str} ~ {end_str} ({row['run_length']}ì¼)\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"collapsed":true,"id":"DuGJWGEWWc_I"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","=== ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ ===\n","  2023-03-01 ~ 2023-03-09 (9ì¼)\n","  2024-03-03 ~ 2024-03-13 (11ì¼)\n","\n","=== ë‹´í•˜ ===\n","  2023-03-02 ~ 2023-03-09 (8ì¼)\n","  2024-03-04 ~ 2024-03-12 (9ì¼)\n","\n","=== ë¼ê·¸ë¡œíƒ€ ===\n","  2023-03-02 ~ 2023-03-09 (8ì¼)\n","  2024-03-04 ~ 2024-03-12 (9ì¼)\n","\n","=== ë¯¸ë¼ì‹œì•„ ===\n","  2023-03-02 ~ 2023-03-09 (8ì¼)\n","  2024-03-04 ~ 2024-03-12 (9ì¼)\n","\n","=== ì—°íšŒì¥ ===\n","  2023-03-01 ~ 2023-03-09 (9ì¼)\n","  2024-03-04 ~ 2024-03-13 (10ì¼)\n","\n","=== ì¹´í˜í…Œë¦¬ì•„ ===\n","  2023-03-02 ~ 2023-03-09 (8ì¼)\n","  2024-03-04 ~ 2024-03-13 (10ì¼)\n","\n","=== í¬ë ˆìŠ¤íŠ¸ë¦¿ ===\n","  2023-03-02 ~ 2023-03-11 (10ì¼)\n","  2023-03-13 ~ 2023-03-31 (19ì¼)\n","  2023-04-03 ~ 2023-04-06 (4ì¼)\n","  2023-04-10 ~ 2023-04-13 (4ì¼)\n","  2023-04-17 ~ 2023-04-20 (4ì¼)\n","  2023-08-28 ~ 2023-09-01 (5ì¼)\n","  2023-09-04 ~ 2023-09-07 (4ì¼)\n","  2023-09-11 ~ 2023-09-15 (5ì¼)\n","  2023-09-18 ~ 2023-09-22 (5ì¼)\n","  2023-11-27 ~ 2023-12-05 (9ì¼)\n","  2024-03-04 ~ 2024-03-26 (23ì¼)\n","  2024-04-29 ~ 2024-05-02 (4ì¼)\n","  2024-05-20 ~ 2024-05-23 (4ì¼)\n","  2024-05-27 ~ 2024-05-31 (5ì¼)\n","  2024-06-10 ~ 2024-06-13 (4ì¼)\n","\n","=== í™”ë‹´ìˆ²ì£¼ë§‰ ===\n","  2023-01-01 ~ 2023-03-30 (89ì¼)\n","  2023-11-27 ~ 2024-03-28 (123ì¼)\n","\n","=== í™”ë‹´ìˆ²ì¹´í˜ ===\n","  2023-01-01 ~ 2023-03-30 (89ì¼)\n","  2023-11-27 ~ 2024-03-28 (123ì¼)\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-3391356828.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n"]}],"source":["zero_periods = find_store_zero_runs(\n","    df,  # ì›ë³¸ ë°ì´í„°í”„ë ˆì„\n","    date_col='ì˜ì—…ì¼ì',\n","    store_col='ì˜ì—…ì¥ëª…',\n","    sales_col='ë§¤ì¶œìˆ˜ëŸ‰',\n","    min_run=4,\n","    treat_missing_as_zero=True\n",")\n","\n","print_zero_runs_by_store(zero_periods)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pEFJtUd4YFxJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-303728899.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","# --- 1) ì˜ì—…ì¥ë³„ \"ì—°ì† 0(\u003e=min_runì¼)\" êµ¬ê°„ ìº˜ë¦°ë” ìƒì„± ---\n","def _find_store_zero_runs_calendar(df: pd.DataFrame,\n","                                   date_col='ì˜ì—…ì¼ì',\n","                                   store_col='ì˜ì—…ì¥ëª…',\n","                                   sales_col='ë§¤ì¶œìˆ˜ëŸ‰',\n","                                   min_run=7,\n","                                   treat_missing_as_zero=True) -\u003e pd.DataFrame:\n","    \"\"\"\n","    ë°˜í™˜: [ì˜ì—…ì¥ëª…, ds, íœ´ë¬´ì¼] (ì—°ì† 0 êµ¬ê°„ì˜ ëª¨ë“  ë‚ ì§œì— íœ´ë¬´ì¼=1)\n","    \"\"\"\n","    df = df.copy()\n","    df[date_col] = pd.to_datetime(df[date_col])\n","\n","    # ì˜ì—…ì¥Ã—ë‚ ì§œ ì´ë§¤ì¶œ(ëª¨ë“  ë©”ë‰´ í•©)\n","    daily = (df.groupby([store_col, date_col], as_index=False)[sales_col]\n","               .sum()\n","               .rename(columns={sales_col: 'store_sales'}))\n","\n","    rows = []\n","    for store, g in daily.groupby(store_col, sort=False):\n","        g = g.sort_values(date_col)\n","\n","        # ì¼ ë‹¨ìœ„ ì¸ë±ìŠ¤ í™•ë³´\n","        if treat_missing_as_zero:\n","            full_idx = pd.date_range(g[date_col].min(), g[date_col].max(), freq='D')\n","            s = (g.set_index(date_col)['store_sales']\n","                   .reindex(full_idx)\n","                   .fillna(0.0))\n","            s.index.name = date_col\n","        else:\n","            s = g.set_index(date_col)['store_sales'].copy()\n","\n","        z = (s == 0).astype(int)\n","        if z.sum() == 0:\n","            continue\n","\n","        # run ë¶„í• \n","        run_id = (z.diff().fillna(z.iloc[0]) != 0).cumsum()\n","        tmp = (\n","            pd.DataFrame({'flag_zero': z, 'run_id': run_id})\n","            .groupby('run_id', group_keys=False)\n","            .apply(lambda d: pd.Series({\n","                'flag_zero': d['flag_zero'].iloc[0],\n","                'start':     d.index.min(),\n","                'end':       d.index.max(),\n","                'length':    len(d)\n","            }))\n","        )\n","\n","        # 0êµ¬ê°„ + ê¸¸ì´ ì¡°ê±´\n","        tmp = tmp[(tmp['flag_zero'] == 1) \u0026 (tmp['length'] \u003e= min_run)]\n","        if tmp.empty:\n","            continue\n","\n","        # ë‚ ì§œ ìº˜ë¦°ë”ë¡œ í™•ì¥\n","        for _, r in tmp.iterrows():\n","            days = pd.date_range(r['start'], r['end'], freq='D')\n","            rows.append(pd.DataFrame({\n","                store_col: store,\n","                'ds': days,\n","                'íœ´ë¬´ì¼': 1\n","            }))\n","\n","    if rows:\n","        off_cal = pd.concat(rows, ignore_index=True).drop_duplicates([store_col, 'ds'])\n","    else:\n","        off_cal = pd.DataFrame(columns=[store_col, 'ds', 'íœ´ë¬´ì¼'])\n","    return off_cal\n","\n","\n","# --- 2) ê¸°ì¡´ íœ´ë¬´ì¼ ì‚­ì œ í›„, ìƒˆ ê·œì¹™ìœ¼ë¡œ ìƒì„± ---\n","def rebuild_offdays(df: pd.DataFrame,\n","                    date_col='ì˜ì—…ì¼ì',\n","                    store_col='ì˜ì—…ì¥ëª…',\n","                    menu_col='ë©”ë‰´ëª…',\n","                    sales_col='ë§¤ì¶œìˆ˜ëŸ‰',\n","                    min_run=7,\n","                    treat_missing_as_zero=True,\n","                    ) -\u003e pd.DataFrame:\n","    \"\"\"\n","    ê¸°ì¡´ dfì˜ íœ´ë¬´ì¼ ì»¬ëŸ¼ì„ ì œê±°í•˜ê³ , ì—°ì†0(\u003e=min_run) + íŠ¹ì • ë§¤ì¥ ì›”ìš”ì¼ ê·œì¹™ìœ¼ë¡œ ìƒˆë¡œ ìƒì„±.\n","    ë°˜í™˜: íœ´ë¬´ì¼(0/1)ì´ ìƒˆë¡œ ìƒì„±ëœ df\n","    \"\"\"\n","    out = df.copy()\n","    out[date_col] = pd.to_datetime(out[date_col])\n","\n","    # 0) ê¸°ì¡´ íœ´ë¬´ì¼ ì»¬ëŸ¼ ì œê±°\n","    if 'íœ´ë¬´ì¼' in out.columns:\n","        out = out.drop(columns=['íœ´ë¬´ì¼'])\n","\n","    # 1) ì—°ì† 0 êµ¬ê°„ ìº˜ë¦°ë” ìƒì„±\n","    off_cal = _find_store_zero_runs_calendar(out, date_col, store_col, sales_col,\n","                                             min_run=min_run,\n","                                             treat_missing_as_zero=treat_missing_as_zero)\n","\n","    # 2) ê¸°ë³¸ 0ìœ¼ë¡œ ì´ˆê¸°í™” í›„, ìº˜ë¦°ë” ë³‘í•©ìœ¼ë¡œ 1 ì„¸íŒ…\n","    out['íœ´ë¬´ì¼'] = 0\n","    if not off_cal.empty:\n","        off_cal = off_cal.copy()\n","        off_cal['ds'] = pd.to_datetime(off_cal['ds'])\n","        out = out.merge(off_cal[[store_col, 'ds', 'íœ´ë¬´ì¼']].rename(columns={'íœ´ë¬´ì¼':'íœ´ë¬´ì¼_from_zero'}),\n","                        how='left', left_on=[store_col, date_col], right_on=[store_col, 'ds'])\n","        out['íœ´ë¬´ì¼'] = out['íœ´ë¬´ì¼_from_zero'].fillna(0).astype(int)\n","        out.drop(columns=['ds','íœ´ë¬´ì¼_from_zero'], inplace=True)\n","\n","    # 4) ìµœì¢… ì •ë¦¬: int8ë¡œ ë‹¤ìš´ìºìŠ¤íŒ…(ì„ íƒ)\n","    out['íœ´ë¬´ì¼'] = out['íœ´ë¬´ì¼'].astype('int8')\n","    return out\n","\n","\n","# === ì‹¤í–‰ ì˜ˆì‹œ ===\n","# df = ... (ì›ë³¸)\n","df = rebuild_offdays(\n","    df,\n","    date_col='ì˜ì—…ì¼ì',\n","    store_col='ì˜ì—…ì¥ëª…',\n","    menu_col='ë©”ë‰´ëª…',\n","    sales_col='ë§¤ì¶œìˆ˜ëŸ‰',\n","    min_run=4,\n","    treat_missing_as_zero=True,   # ê²°ì¸¡ ì¼ìë„ 0ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ íƒì§€\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"shivast3YwKf"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Š ìš”ì¼ë³„ ë§¤ì¶œ 0(íœ´ë¬´ì¼ ì•„ë‹˜) ì¹´ìš´íŠ¸\n","weekday_name\n","ì›”    159\n","í™”     12\n","ìˆ˜      6\n","ëª©      9\n","ê¸ˆ      3\n","í†       0\n","ì¼     26\n","Name: count, dtype: int64\n","\n","=== ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ ===\n","weekday_name\n","ì›”    15\n","í™”     2\n","Name: count, dtype: int64\n","\n","=== ë¼ê·¸ë¡œíƒ€ ===\n","weekday_name\n","ì›”    51\n","í™”     2\n","Name: count, dtype: int64\n","\n","=== ì—°íšŒì¥ ===\n","weekday_name\n","ìˆ˜     2\n","ì›”     4\n","ì¼    25\n","í™”     2\n","Name: count, dtype: int64\n","\n","=== í¬ë ˆìŠ¤íŠ¸ë¦¿ ===\n","weekday_name\n","ê¸ˆ     3\n","ëª©     7\n","ìˆ˜     4\n","ì›”    21\n","ì¼     1\n","í™”     6\n","Name: count, dtype: int64\n","\n","=== í™”ë‹´ìˆ²ì£¼ë§‰ ===\n","weekday_name\n","ëª©     1\n","ì›”    34\n","Name: count, dtype: int64\n","\n","=== í™”ë‹´ìˆ²ì¹´í˜ ===\n","weekday_name\n","ëª©     1\n","ì›”    34\n","Name: count, dtype: int64\n"]}],"source":["def count_weekdays_for_non_offday_zero_sales(df, date_col='ì˜ì—…ì¼ì', store_col='ì˜ì—…ì¥ëª…',\n","                                             sales_col='ë§¤ì¶œìˆ˜ëŸ‰', offday_col='íœ´ë¬´ì¼'):\n","    \"\"\"\n","    íœ´ë¬´ì¼ì´ ì•„ë‹Œë° ë§¤ì¶œ 0ì¸ ë‚ ì§œì˜ ìš”ì¼ ë¶„í¬ë¥¼ ì¹´ìš´íŠ¸\n","    \"\"\"\n","    df = df.copy()\n","    df[date_col] = pd.to_datetime(df[date_col])\n","\n","    # 1) ì˜ì—…ì¥ Ã— ë‚ ì§œ ì´ë§¤ì¶œ\n","    daily_sales = (\n","        df.groupby([store_col, date_col], as_index=False)[sales_col]\n","          .sum()\n","          .rename(columns={sales_col: 'store_sales'})\n","    )\n","\n","    # 2) íœ´ë¬´ì¼ ì •ë³´ ë³‘í•©\n","    if offday_col in df.columns:\n","        daily_sales = daily_sales.merge(\n","            df[[store_col, date_col, offday_col]].drop_duplicates(),\n","            on=[store_col, date_col],\n","            how='left'\n","        )\n","    else:\n","        daily_sales[offday_col] = 0\n","\n","    # 3) ì¡°ê±´ í•„í„°\n","    target = daily_sales[\n","        (daily_sales[offday_col] == 0) \u0026\n","        (daily_sales['store_sales'] == 0)\n","    ].copy()\n","\n","    # 4) ìš”ì¼ ì»¬ëŸ¼ ì¶”ê°€ (0=ì›”, 6=ì¼)\n","    target['weekday'] = target[date_col].dt.weekday\n","\n","    # 5) ìš”ì¼ ì´ë¦„ ë§¤í•‘\n","    weekday_map = {0: 'ì›”', 1: 'í™”', 2: 'ìˆ˜', 3: 'ëª©', 4: 'ê¸ˆ', 5: 'í† ', 6: 'ì¼'}\n","    target['weekday_name'] = target['weekday'].map(weekday_map)\n","\n","    # 6) ìš”ì¼ ì¹´ìš´íŠ¸\n","    weekday_counts = target['weekday_name'].value_counts().reindex(weekday_map.values(), fill_value=0)\n","\n","    return target, weekday_counts\n","\n","\n","# === ì‹¤í–‰ ì˜ˆì‹œ ===\n","target_df, weekday_counts = count_weekdays_for_non_offday_zero_sales(df)\n","\n","print(\"ğŸ“Š ìš”ì¼ë³„ ë§¤ì¶œ 0(íœ´ë¬´ì¼ ì•„ë‹˜) ì¹´ìš´íŠ¸\")\n","print(weekday_counts)\n","\n","# í•„ìš”í•˜ë©´ ì˜ì—…ì¥ë³„ ìƒì„¸ ì¶œë ¥\n","for store, group in target_df.groupby('ì˜ì—…ì¥ëª…'):\n","    print(f\"\\n=== {store} ===\")\n","    print(group['weekday_name'].value_counts().sort_index())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lz00khSjd4S9"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== ì—…ì¥ë³„Â·ì›”ë³„ 0ë§¤ì¶œ ìš”ì¼ ì¹´ìš´íŠ¸ (íœ´ë¬´ì¼ ì œì™¸) ===\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"pivot_all\",\n  \"rows\": 74,\n  \"fields\": [\n    {\n      \"column\": \"\\uc6d4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          2,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud654\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc218\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ubaa9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uae08\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud1a0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc77c\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"pivot_all"},"text/html":["\n","  \u003cdiv id=\"df-ea2fb9cf-f7ac-4410-953c-8a2a6ccde571\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eweekday_name\u003c/th\u003e\n","      \u003cth\u003eì›”\u003c/th\u003e\n","      \u003cth\u003eí™”\u003c/th\u003e\n","      \u003cth\u003eìˆ˜\u003c/th\u003e\n","      \u003cth\u003eëª©\u003c/th\u003e\n","      \u003cth\u003eê¸ˆ\u003c/th\u003e\n","      \u003cth\u003eí† \u003c/th\u003e\n","      \u003cth\u003eì¼\u003c/th\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eì˜ì—…ì¥ëª…\u003c/th\u003e\n","      \u003cth\u003emonth\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth rowspan=\"5\" valign=\"top\"\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ\u003c/th\u003e\n","      \u003cth\u003e2023-03\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2023-04\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2023-07\u003c/th\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2023-09\u003c/th\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2023-10\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth rowspan=\"5\" valign=\"top\"\u003eí™”ë‹´ìˆ²ì¹´í˜\u003c/th\u003e\n","      \u003cth\u003e2023-10\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2023-11\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2024-04\u003c/th\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2024-05\u003c/th\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2024-06\u003c/th\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e74 rows Ã— 7 columns\u003c/p\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea2fb9cf-f7ac-4410-953c-8a2a6ccde571')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-ea2fb9cf-f7ac-4410-953c-8a2a6ccde571 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ea2fb9cf-f7ac-4410-953c-8a2a6ccde571');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","    \u003cdiv id=\"df-19482349-3087-4591-8b2d-1430e05cb159\"\u003e\n","      \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-19482349-3087-4591-8b2d-1430e05cb159')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","      \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() =\u003e {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-19482349-3087-4591-8b2d-1430e05cb159 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","\n","  \u003cdiv id=\"id_e72106fb-04d8-4dcf-bff0-91d4c7992ca8\"\u003e\n","    \u003cstyle\u003e\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    \u003c/style\u003e\n","    \u003cbutton class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_all')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","    \u003cscript\u003e\n","      (() =\u003e {\n","      const buttonEl =\n","        document.querySelector('#id_e72106fb-04d8-4dcf-bff0-91d4c7992ca8 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () =\u003e {\n","        google.colab.notebook.generateWithVariable('pivot_all');\n","      }\n","      })();\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["weekday_name        ì›”  í™”  ìˆ˜  ëª©  ê¸ˆ  í†   ì¼\n","ì˜ì—…ì¥ëª…       month                       \n","ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ 2023-03  1  1  0  0  0  0  0\n","           2023-04  1  0  0  0  0  0  0\n","           2023-07  2  0  0  0  0  0  0\n","           2023-09  4  0  0  0  0  0  0\n","           2023-10  1  0  0  0  0  0  0\n","...                .. .. .. .. .. .. ..\n","í™”ë‹´ìˆ²ì¹´í˜      2023-10  1  0  0  0  0  0  0\n","           2023-11  1  0  0  0  0  0  0\n","           2024-04  5  0  0  0  0  0  0\n","           2024-05  3  0  0  0  0  0  0\n","           2024-06  2  0  0  0  0  0  0\n","\n","[74 rows x 7 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","\n","def count_zero_sales_by_store_month_weekday_excluding_offdays(\n","    df: pd.DataFrame,\n","    date_col: str = 'ì˜ì—…ì¼ì',\n","    store_col: str = 'ì˜ì—…ì¥ëª…',\n","    sales_col: str = 'ë§¤ì¶œìˆ˜ëŸ‰',\n","    offday_col: str = 'íœ´ë¬´ì¼',\n","    month_format: str = '%Y-%m'\n","):\n","    \"\"\"\n","    íœ´ë¬´ì¼ ì œì™¸ í›„, ì—…ì¥ë³„Â·ì›”ë³„ 'ë§¤ì¶œìˆ˜ëŸ‰=0'ì¸ ë‚ ì§œì˜ ìš”ì¼ ì¹´ìš´íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    df = df.copy()\n","    df[date_col] = pd.to_datetime(df[date_col])\n","\n","    # 1) ì˜ì—…ì¥Ã—ë‚ ì§œ ì´ë§¤ì¶œ (ëª¨ë“  ë©”ë‰´ í•©)\n","    daily_sales = (\n","        df.groupby([store_col, date_col], as_index=False)[sales_col]\n","          .sum()\n","          .rename(columns={sales_col: 'store_sales'})\n","    )\n","\n","    # 2) íœ´ë¬´ì¼ ì •ë³´ ë³‘í•© \u0026 ì œì™¸\n","    if offday_col in df.columns:\n","        offday_info = (\n","            df[[store_col, date_col, offday_col]]\n","            .drop_duplicates([store_col, date_col])\n","        )\n","        daily_sales = daily_sales.merge(offday_info, on=[store_col, date_col], how='left')\n","        daily_sales = daily_sales[daily_sales[offday_col].fillna(0).astype(int) == 0]\n","\n","    # 3) 0ë§¤ì¶œ ë‚ ì§œë§Œ í•„í„°\n","    zero_sales = daily_sales[daily_sales['store_sales'] == 0].copy()\n","\n","    # 4) ìš”ì¼ / ì›” íŒŒìƒ\n","    zero_sales['weekday'] = zero_sales[date_col].dt.weekday  # ì›”=0 â€¦ ì¼=6\n","    weekday_map = {0:'ì›”', 1:'í™”', 2:'ìˆ˜', 3:'ëª©', 4:'ê¸ˆ', 5:'í† ', 6:'ì¼'}\n","    zero_sales['weekday_name'] = zero_sales['weekday'].map(weekday_map)\n","    zero_sales['month'] = zero_sales[date_col].dt.strftime(month_format)\n","\n","    # 5) Long í˜•íƒœ ì§‘ê³„\n","    counts_long = (\n","        zero_sales.groupby([store_col, 'month', 'weekday_name'], as_index=False)\n","                  .size()\n","                  .rename(columns={'size':'zero_days'})\n","    )\n","\n","    # 6) Pivot í˜•íƒœ\n","    counts_pivot = (\n","        counts_long.pivot_table(index=[store_col, 'month'],\n","                                columns='weekday_name',\n","                                values='zero_days',\n","                                aggfunc='sum',\n","                                fill_value=0)\n","                    .reindex(columns=['ì›”','í™”','ìˆ˜','ëª©','ê¸ˆ','í† ','ì¼'], fill_value=0)\n","                    .sort_index()\n","    )\n","\n","    return counts_pivot, counts_long\n","\n","\n","# ì‚¬ìš© ì˜ˆì‹œ\n","pivot_all, long_all = count_zero_sales_by_store_month_weekday_excluding_offdays(df)\n","\n","print(\"=== ì—…ì¥ë³„Â·ì›”ë³„ 0ë§¤ì¶œ ìš”ì¼ ì¹´ìš´íŠ¸ (íœ´ë¬´ì¼ ì œì™¸) ===\")\n","display(pivot_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EIIrbBPCfOIz"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== íœ´ë¬´ì¼ ì§€ì • ê³„íš ===\n","ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ (2023-09): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-02): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-03): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-04): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-05): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-06): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-07): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-08): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-09): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2023-11): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2024-04): ì›”\n","ë¼ê·¸ë¡œíƒ€ (2024-05): ì›”\n","ì—°íšŒì¥ (2023-03): ì¼\n","ì—°íšŒì¥ (2023-04): ì¼\n","í¬ë ˆìŠ¤íŠ¸ë¦¿ (2023-05): ì›”\n","í¬ë ˆìŠ¤íŠ¸ë¦¿ (2023-10): ì›”\n","í¬ë ˆìŠ¤íŠ¸ë¦¿ (2023-11): ì›”\n","í¬ë ˆìŠ¤íŠ¸ë¦¿ (2024-04): ì›”\n","í™”ë‹´ìˆ²ì£¼ë§‰ (2023-04): ì›”\n","í™”ë‹´ìˆ²ì£¼ë§‰ (2023-05): ì›”\n","í™”ë‹´ìˆ²ì£¼ë§‰ (2023-06): ì›”\n","í™”ë‹´ìˆ²ì£¼ë§‰ (2023-07): ì›”\n","í™”ë‹´ìˆ²ì£¼ë§‰ (2023-08): ì›”\n","í™”ë‹´ìˆ²ì£¼ë§‰ (2023-09): ì›”\n","í™”ë‹´ìˆ²ì£¼ë§‰ (2024-04): ì›”\n","í™”ë‹´ìˆ²ì£¼ë§‰ (2024-05): ì›”\n","í™”ë‹´ìˆ²ì¹´í˜ (2023-04): ì›”\n","í™”ë‹´ìˆ²ì¹´í˜ (2023-05): ì›”\n","í™”ë‹´ìˆ²ì¹´í˜ (2023-06): ì›”\n","í™”ë‹´ìˆ²ì¹´í˜ (2023-07): ì›”\n","í™”ë‹´ìˆ²ì¹´í˜ (2023-08): ì›”\n","í™”ë‹´ìˆ²ì¹´í˜ (2023-09): ì›”\n","í™”ë‹´ìˆ²ì¹´í˜ (2024-04): ì›”\n","í™”ë‹´ìˆ²ì¹´í˜ (2024-05): ì›”\n"]}],"source":["def assign_offdays_by_zero_sales(pivot_table, threshold=3):\n","    \"\"\"\n","    pivot_table: count_zero_sales_by_store_month_weekday_excluding_offdays()ì˜ pivot_all ê²°ê³¼\n","    threshold: í•´ë‹¹ ìš”ì¼ì´ ì´ íšŸìˆ˜ ì´ìƒì´ë©´ íœ´ë¬´ì¼ë¡œ ì§€ì •\n","    \"\"\"\n","    offday_records = []\n","\n","    for (store, month), row in pivot_table.iterrows():\n","        # 3íšŒ ì´ìƒì¸ ìš”ì¼ ì¶”ì¶œ\n","        offdays = [day for day, cnt in row.items() if cnt \u003e= threshold]\n","        if offdays:\n","            offday_records.append({\n","                'ì˜ì—…ì¥ëª…': store,\n","                'ì›”': month,\n","                'íœ´ë¬´ìš”ì¼': offdays\n","            })\n","\n","    return pd.DataFrame(offday_records)\n","\n","\n","# ì‚¬ìš© ì˜ˆì‹œ\n","pivot_all, _ = count_zero_sales_by_store_month_weekday_excluding_offdays(df)\n","offday_plan_df = assign_offdays_by_zero_sales(pivot_all, threshold=3)\n","\n","print(\"=== íœ´ë¬´ì¼ ì§€ì • ê³„íš ===\")\n","for _, row in offday_plan_df.iterrows():\n","    print(f\"{row['ì˜ì—…ì¥ëª…']} ({row['ì›”']}): {', '.join(row['íœ´ë¬´ìš”ì¼'])}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FUNBIaBU3LC6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def add_custom_offday_only(\n","    df: pd.DataFrame,\n","    date_col: str = 'ì˜ì—…ì¼ì',\n","    store_col: str = 'ì˜ì—…ì¥ëª…',\n","    offday_col: str = 'íœ´ë¬´ì¼',      # 0=ì˜ì—…, 1=íœ´ë¬´\n","    out_col: str = 'custom_offday'\n",") -\u003e pd.DataFrame:\n","    \"\"\"\n","    ê¸°ì¡´ íœ´ë¬´ì¼(offday_col)==0(ì˜ì—…ì¼)ì—ì„œë§Œ ê·œì¹™ì„ ì ìš©í•´ custom_offday=1ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.\n","    ê·œì¹™:\n","      - í¬ë ˆìŠ¤íŠ¸ë¦¿: 4,5,9,10,11ì›” \u0026 ì›”ìš”ì¼(weekday=0)\n","      - ì—°íšŒì¥: ì¼ìš”ì¼(weekday=6)\n","      - í™”ë‹´ìˆ²ì£¼ë§‰/í™”ë‹´ìˆ²ì¹´í˜/ë¼ê·¸ë¡œíƒ€/ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ: ì›”ìš”ì¼(weekday=0)\n","    \"\"\"\n","    if offday_col not in df.columns:\n","        raise KeyError(f\"'{offday_col}' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. (0=ì˜ì—…, 1=íœ´ë¬´)\")\n","\n","    out = df.copy()\n","    out[date_col] = pd.to_datetime(out[date_col])\n","\n","    # --- íœ´ë¬´ì¼ ì»¬ëŸ¼ ì •ê·œí™” (bool/ë¬¸ìì—´/ìˆ«ì ëª¨ë‘ ì²˜ë¦¬) ---\n","    off = out[offday_col]\n","    if off.dtype == bool or set(off.dropna().unique()) \u003c= {True, False}:\n","        off = off.astype(int)\n","    elif off.dtype == 'O':  # 'holiday'/'non-holiday' ë“± ë¬¸ìì—´\n","        off = (off.replace({'holiday': 1, 'non-holiday': 0})\n","                 .pipe(pd.to_numeric, errors='coerce')\n","                 .fillna(0).astype(int))\n","    else:\n","        off = pd.to_numeric(off, errors='coerce').fillna(0).astype(int)\n","    # ì˜ì—… ì¤‘(íœ´ë¬´ ì•„ë‹˜) ì¡°ê±´\n","    is_op = (off == 0).values\n","\n","    # --- íŒŒìƒ ---\n","    out['weekday'] = out[date_col].dt.weekday  # ì›”=0 â€¦ ì¼=6\n","    out['month']   = out[date_col].dt.month\n","\n","    # --- ê¸°ë³¸ 0 ---\n","    custom = np.zeros(len(out), dtype=np.int8)\n","\n","    # 1) í¬ë ˆìŠ¤íŠ¸ë¦¿: 4,5,9,10,11ì›” \u0026 ì›”ìš”ì¼ \u0026 ì˜ì—… ì¤‘\n","    mask = (\n","        (out[store_col] == 'í¬ë ˆìŠ¤íŠ¸ë¦¿') \u0026\n","        (out['month'].isin([4,5,9,10,11])) \u0026\n","        (out['weekday'] == 0) \u0026 is_op\n","    )\n","    custom = np.where(mask, 1, custom)\n","\n","    # 2) ì—°íšŒì¥: ì¼ìš”ì¼ \u0026 ì˜ì—… ì¤‘\n","    mask = (\n","        (out[store_col] == 'ì—°íšŒì¥') \u0026\n","        (out['weekday'] == 6) \u0026 is_op\n","    )\n","    custom = np.where(mask, 1, custom)\n","\n","    # 3) í™”ë‹´ìˆ²ì£¼ë§‰: ì›”ìš”ì¼ \u0026 ì˜ì—… ì¤‘\n","    mask = (\n","        (out[store_col] == 'í™”ë‹´ìˆ²ì£¼ë§‰') \u0026\n","        (out['weekday'] == 0) \u0026 is_op\n","    )\n","    custom = np.where(mask, 1, custom)\n","\n","    # 4) í™”ë‹´ìˆ²ì¹´í˜: ì›”ìš”ì¼ \u0026 ì˜ì—… ì¤‘\n","    mask = (\n","        (out[store_col] == 'í™”ë‹´ìˆ²ì¹´í˜') \u0026\n","        (out['weekday'] == 0) \u0026 is_op\n","    )\n","    custom = np.where(mask, 1, custom)\n","\n","    # 5) ë¼ê·¸ë¡œíƒ€: ì›”ìš”ì¼ \u0026 ì˜ì—… ì¤‘\n","    mask = (\n","        (out[store_col] == 'ë¼ê·¸ë¡œíƒ€') \u0026\n","        (out['weekday'] == 0) \u0026 is_op\n","    )\n","    custom = np.where(mask, 1, custom)\n","\n","    # 6) ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ: ì›”ìš”ì¼ \u0026 ì˜ì—… ì¤‘\n","    mask = (\n","        (out[store_col] == 'ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ') \u0026\n","        (out['weekday'] == 0) \u0026 is_op\n","    )\n","    custom = np.where(mask, 1, custom)\n","\n","    out[out_col] = custom.astype('int8')\n","\n","    # ì •ë¦¬\n","    out.drop(columns=['weekday','month'], inplace=True)\n","    return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oxm_SYRQ3Mzs"},"outputs":[],"source":["df = add_custom_offday_only(df, date_col='ì˜ì—…ì¼ì', store_col='ì˜ì—…ì¥ëª…', offday_col='íœ´ë¬´ì¼', out_col='custom_offday')"]},{"cell_type":"markdown","metadata":{"id":"Og0lIdjRyLTa"},"source":["ìš”ì¼ë³„ 0ë§¤ì¶œ ë¹„ìœ¨ì´ 90% ì´ìƒì´ê±°ë‚˜ íŒë§¤ ì¤‘ì§€ ê¸°ê°„ì¼ ì‹œì— ì´ìƒì¹˜ ë³´ê°„ì—ì„œ ì œì™¸"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Td5zBFtOxfWj"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from statsmodels.tsa.seasonal import STL, seasonal_decompose\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ìš”ì¼ë³„ 0ë§¤ì¶œ ë¹„ìœ¨ë¡œ 'ì •ê¸° íœ´ë¬´ ìš”ì¼' ìë™ íŒì •\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def _detect_weekly_off_days_from_series(ts: pd.Series, min_weeks=8, tau=0.8):\n","    tmp = pd.DataFrame({\"y\": ts})\n","    tmp[\"weekday\"] = tmp.index.weekday\n","    weekly_off = set()\n","    for w in range(7):\n","        sub = tmp.loc[tmp[\"weekday\"] == w, \"y\"]\n","        n_obs = sub.notna().sum()\n","        if n_obs \u003c min_weeks:\n","            continue\n","        zero_rate = (sub.fillna(0) == 0).mean()\n","        if zero_rate \u003e= tau:\n","            weekly_off.add(w)\n","    return weekly_off\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ë‹¨ì¼ (ì˜ì—…ì¥ëª…, ë©”ë‰´ëª…) ê·¸ë£¹ ì²­ì†Œ + ê³ ë¦½ 0/ì´ìƒì¹˜ ì„ í˜•ë³´ê°„\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def _clean_one_group(subset: pd.DataFrame,\n","                     period=7, max_interp_gap=None,  # Noneì´ë©´ ì œí•œ ì—†ì´ ë³´ê°„\n","                     min_weeks=8, tau=0.8,\n","                     offday_col='íœ´ë¬´ì¼', custom_off_col='custom_offday') -\u003e pd.Series | None:\n","    \"\"\"\n","    subset: ë‹¨ì¼ (ì˜ì—…ì¥ëª…, ë©”ë‰´ëª…) ë°ì´í„°í”„ë ˆì„\n","            ë°˜ë“œì‹œ í¬í•¨: ['ì˜ì—…ì¼ì','ë§¤ì¶œìˆ˜ëŸ‰','is_zero_sales_period']\n","            ì„ íƒ í¬í•¨: ['íœ´ë¬´ì¼','custom_offday'] (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ê°„ì£¼)\n","    return: ì²­ì†Œ ì™„ë£Œëœ ì‹œê³„ì—´ (DatetimeIndex, float)\n","    \"\"\"\n","    # 1) ì¼ì ì§‘ê³„ \u0026 ì •ë ¬\n","    g = subset.groupby('ì˜ì—…ì¼ì').agg(\n","        ë§¤ì¶œìˆ˜ëŸ‰=('ë§¤ì¶œìˆ˜ëŸ‰', 'sum'),\n","        is_zero=('is_zero_sales_period', 'max'),\n","        íœ´ë¬´ì¼=(offday_col, 'max') if offday_col in subset.columns else ('ì˜ì—…ì¼ì', lambda x: 0),\n","        custom_offday=(custom_off_col, 'max') if custom_off_col in subset.columns else ('ì˜ì—…ì¼ì', lambda x: 0),\n","    ).sort_index()\n","\n","    # 2) ì „ì²´ ì¼ì ì¸ë±ìŠ¤ë¡œ ì¬ìƒ‰ì¸\n","    full_idx = pd.date_range(g.index.min(), g.index.max(), freq='D')\n","    g = g.reindex(full_idx)\n","\n","    ts = g['ë§¤ì¶œìˆ˜ëŸ‰']\n","    structural_zero = g['is_zero'].fillna(0).astype(int).eq(1)\n","    offday_mask = (g['íœ´ë¬´ì¼'].fillna(0).astype(int).eq(1)) | (g['custom_offday'].fillna(0).astype(int).eq(1))\n","\n","    if len(ts) \u003c period * 2:\n","        return None  # ë°ì´í„° ë¶€ì¡±\n","\n","    # 3) ì •ê¸° íœ´ë¬´ ìš”ì¼ íƒì§€ (íŒë§¤ ê¸°ë¡ ê¸°ë°˜)\n","    weekly_off_days = _detect_weekly_off_days_from_series(ts, min_weeks=min_weeks, tau=tau)\n","    weekday = pd.Series(ts.index.weekday, index=ts.index)\n","    weekly_off_mask = weekday.isin(weekly_off_days) \u0026 (ts.isna() | ts.eq(0))\n","\n","    # 4) í•˜ë“œ 0(ë³´ê°„ ê¸ˆì§€) ì •ì˜\n","    hard_zero_mask = structural_zero | weekly_off_mask | offday_mask\n","\n","    # 5) STL ë¶„í•´ìš© ì‹œê³„ì—´: í•˜ë“œ0ëŠ” NaNìœ¼ë¡œ ë‘ê³ , ì§§ì€ ê²°ì¸¡ë§Œ ì œí•œ ë³´ê°„\n","    ts_for_decomp = ts.copy()\n","    ts_for_decomp[hard_zero_mask] = np.nan\n","    ts_for_decomp = ts_for_decomp.interpolate(\n","        method='linear',\n","        limit=max_interp_gap,  # Noneì´ë©´ ì œí•œ ì—†ìŒ\n","        limit_direction='both'\n","    )\n","\n","    # 6) ë¶„í•´(STLâ†’fallback)\n","    try:\n","        stl = STL(ts_for_decomp.dropna(), period=period, robust=True)\n","        resid = stl.fit().resid.reindex(ts_for_decomp.index)\n","    except Exception:\n","        decomp = seasonal_decompose(ts_for_decomp.dropna(), model='additive', period=period)\n","        resid = decomp.resid.reindex(ts_for_decomp.index)\n","\n","    # 7) ì´ìƒì¹˜ ë§ˆìŠ¤í¬(3Ïƒ ì´ˆê³¼), í•˜ë“œ0ëŠ” ì œì™¸\n","    sigma = resid.std(skipna=True)\n","    outlier_mask = pd.Series(False, index=ts.index)\n","    if pd.notna(sigma) and sigma \u003e 0:\n","        outlier_mask = resid.abs() \u003e (3 * sigma)\n","        outlier_mask = outlier_mask.fillna(False) \u0026 (~hard_zero_mask)\n","\n","    # 8) 'ê³ ë¦½ëœ 0ë§¤ì¶œ' ë§ˆìŠ¤í¬: ì •ê¸°/ì¥ê¸°/íœ´ë¬´ê°€ ì•„ë‹Œë° ê°’ì´ 0\n","    isolated_zero_mask = ts.eq(0) \u0026 (~hard_zero_mask)\n","\n","    # 9) ë³´ê°„ ëŒ€ìƒ: ê³ ë¦½ëœ 0 âˆª ì´ìƒì¹˜\n","    target_mask = (isolated_zero_mask | outlier_mask)\n","\n","    # 10) ì„ í˜•ë³´ê°„ ìˆ˜í–‰\n","    ts_clean = ts.copy()\n","    guard = ts_clean[hard_zero_mask].copy()           # í•˜ë“œ0 ê°’ ë³´ì¡´(ëŒ€ê°œ 0 ë˜ëŠ” NaN)\n","    ts_clean[target_mask] = np.nan                    # ë³´ê°„ ëŒ€ìƒë§Œ NaNìœ¼ë¡œ\n","    ts_final = ts_clean.interpolate(\n","        method='linear',\n","        limit=max_interp_gap,                         # Noneì´ë©´ ì „ë¶€ ì„ í˜•ë³´ê°„\n","        limit_direction='both'\n","    )\n","    # ëë‹¨ ë“± ë‚¨ì€ ê²°ì¸¡ ë³´ì™„ (ì„ í˜•ë³´ê°„ ë¶ˆê°€ êµ¬ê°„)\n","    if ts_final.isna().any():\n","        ts_final = ts_final.fillna(method='ffill').fillna(method='bfill')\n","\n","    # 11) í•˜ë“œ0 ë³µì›(íœ´ë¬´ ê²°ì¸¡ì€ 0 ê³ ì •)\n","    ts_final[hard_zero_mask] = guard.fillna(0)\n","\n","    return ts_final\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ì „ì²´ dfì— ëŒ€í•´ ê·¸ë£¹ë³„ ì²­ì†Œ ìˆ˜í–‰\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def clean_all_series(df: pd.DataFrame,\n","                     store_col=\"ì˜ì—…ì¥ëª…\", menu_col=\"ë©”ë‰´ëª…\",\n","                     date_col=\"ì˜ì—…ì¼ì\", value_col=\"ë§¤ì¶œìˆ˜ëŸ‰\",\n","                     zero_flag_col=\"is_zero_sales_period\",\n","                     offday_col=\"íœ´ë¬´ì¼\", custom_off_col=\"custom_offday\",\n","                     period=7, max_interp_gap=None, min_weeks=8, tau=0.8):\n","    \"\"\"\n","    return:\n","      cleaned_series_dict: { \"ì˜ì—…ì¥_ë©”ë‰´\": pd.Series(DatetimeIndex) }\n","      failed_keys: ë¦¬ìŠ¤íŠ¸ (ì—ëŸ¬/ë°ì´í„°ë¶€ì¡± ë“±ìœ¼ë¡œ ìŠ¤í‚µëœ í‚¤)\n","    \"\"\"\n","    cleaned_series_dict = {}\n","    failed_keys = []\n","\n","    cols = [store_col, menu_col, date_col, value_col, zero_flag_col]\n","    if offday_col in df.columns:\n","        cols.append(offday_col)\n","    if custom_off_col in df.columns:\n","        cols.append(custom_off_col)\n","\n","    d = df[cols].copy()\n","    d[date_col] = pd.to_datetime(d[date_col])\n","\n","    for (store, menu), subset in d.groupby([store_col, menu_col], sort=False):\n","        key = f\"{store}_{menu}\"\n","        try:\n","            # ì—´ ì´ë¦„ í†µì¼\n","            renamer = {\n","                date_col: 'ì˜ì—…ì¼ì',\n","                value_col: 'ë§¤ì¶œìˆ˜ëŸ‰',\n","                zero_flag_col: 'is_zero_sales_period'\n","            }\n","            if offday_col in subset.columns:\n","                renamer[offday_col] = 'íœ´ë¬´ì¼'\n","            if custom_off_col in subset.columns:\n","                renamer[custom_off_col] = 'custom_offday'\n","\n","            sub = subset.rename(columns=renamer)\n","\n","            ts_clean = _clean_one_group(\n","                sub,\n","                period=period,\n","                max_interp_gap=max_interp_gap,\n","                min_weeks=min_weeks,\n","                tau=tau,\n","                offday_col='íœ´ë¬´ì¼',\n","                custom_off_col='custom_offday'\n","            )\n","            if ts_clean is None:\n","                failed_keys.append((key, \"ë°ì´í„° ë¶€ì¡±\"))\n","                continue\n","            cleaned_series_dict[key] = ts_clean\n","        except Exception as e:\n","            failed_keys.append((key, str(e)))\n","    return cleaned_series_dict, failed_keys\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Zd80RTqjylHI"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-1713288838.py:103: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  ts_final = ts_final.fillna(method='ffill').fillna(method='bfill')\n","/tmp/ipython-input-1713288838.py:103: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  ts_final = ts_final.fillna(method='ffill').fillna(method='bfill')\n","/tmp/ipython-input-1713288838.py:103: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  ts_final = ts_final.fillna(method='ffill').fillna(method='bfill')\n"]},{"name":"stdout","output_type":"stream","text":["ìŠ¤í‚µ: []\n"]}],"source":["# 1) ì²­ì†Œ ì‹¤í–‰\n","cleaned_series_dict, failed_keys = clean_all_series(\n","    df,                           # ì›ë³¸ ë°ì´í„°í”„ë ˆì„\n","    store_col=\"ì˜ì—…ì¥ëª…\",\n","    menu_col=\"ë©”ë‰´ëª…\",\n","    date_col=\"ì˜ì—…ì¼ì\",\n","    value_col=\"ë§¤ì¶œìˆ˜ëŸ‰\",\n","    zero_flag_col=\"is_zero_sales_period\",\n","    offday_col=\"íœ´ë¬´ì¼\",\n","    custom_off_col=\"custom_offday\",\n","    period=7,\n","    max_interp_gap=5,          # ì œí•œ ì—†ì´ ì„ í˜•ë³´ê°„\n","    min_weeks=8,\n","    tau=0.85\n",")\n","print(\"ìŠ¤í‚µ:\", failed_keys[:5])\n","\n","# 2) ë³‘í•© í•¨ìˆ˜\n","def merge_cleaned_series(cleaned_dict: dict) -\u003e pd.DataFrame:\n","    rows = []\n","    for key, series in cleaned_dict.items():\n","        store, menu = key.split(\"_\", 1)\n","        df_tmp = pd.DataFrame({\n","            \"ì˜ì—…ì¥ëª…\": store,\n","            \"ë©”ë‰´ëª…\": menu,\n","            \"ì˜ì—…ì¼ì\": series.index,\n","            \"ë§¤ì¶œìˆ˜ëŸ‰\": series.values\n","        })\n","        rows.append(df_tmp)\n","    return pd.concat(rows, ignore_index=True)\n","\n","# 3) ë³‘í•© + í›„ì²˜ë¦¬\n","df1 = merge_cleaned_series(cleaned_series_dict)\n","df1['ë§¤ì¶œìˆ˜ëŸ‰'] = df1['ë§¤ì¶œìˆ˜ëŸ‰'].clip(lower=0)  # ìŒìˆ˜ ë°©ì§€\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qzS_8559W_c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["íœ´ë¬´ì¼ value counts -\u003e\n","íœ´ë¬´ì¼\n","0    95617\n","1     7059\n","Name: count, dtype: int64\n","custom_offday value counts -\u003e\n","custom_offday\n","0    96633\n","1     6043\n","Name: count, dtype: int64\n","is_zero_sales_period value counts -\u003e\n","is_zero_sales_period\n","0    72818\n","1    29858\n","Name: count, dtype: int64\n"]}],"source":["import pandas as pd\n","\n","def add_off_and_zero_flags(df1: pd.DataFrame,\n","                           df: pd.DataFrame,\n","                           date_col='ì˜ì—…ì¼ì',\n","                           shop_col='ì˜ì—…ì¥ëª…',\n","                           menu_col='ë©”ë‰´ëª…',\n","                           combined_col='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…',\n","                           cols_to_add=('íœ´ë¬´ì¼','custom_offday','is_zero_sales_period'),\n","                           prefer_df1=True):\n","    \"\"\"\n","    dfì˜ ['íœ´ë¬´ì¼','custom_offday','is_zero_sales_period']ë¥¼ df1ì— ë¨¸ì§€í•˜ì—¬ ì¶”ê°€/ë³´ì™„í•©ë‹ˆë‹¤.\n","    - í‚¤: (ì˜ì—…ì¥ëª…, ë©”ë‰´ëª…, ì˜ì—…ì¼ì)\n","    - df1ì— ê¸°ì¡´ ê°’ì´ ìˆìœ¼ë©´ ë³´ì¡´í•˜ê³ , ê²°ì¸¡ë§Œ df ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤ (prefer_df1=True).\n","      Falseë¡œ ì£¼ë©´ dfì˜ ê°’ì„ ìš°ì„  ì ìš©í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    df1 = df1.copy()\n","    df  = df.copy()\n","\n","    # 0) ë‚ ì§œ ì»¬ëŸ¼ í˜• ë³€í™˜\n","    df1[date_col] = pd.to_datetime(df1[date_col])\n","    df[date_col]  = pd.to_datetime(df[date_col])\n","\n","    # 1) (ì˜ì—…ì¥ëª…, ë©”ë‰´ëª…) í™•ë³´\n","    def ensure_shop_menu_columns(d):\n","        if shop_col not in d.columns or menu_col not in d.columns:\n","            if combined_col in d.columns:\n","                sp = d[combined_col].astype(str).str.split('_', n=1, expand=True)\n","                d[shop_col] = sp[0]\n","                d[menu_col] = sp[1] if sp.shape[1] \u003e 1 else ''\n","            else:\n","                raise KeyError(\"ì˜ì—…ì¥/ë©”ë‰´ ì»¬ëŸ¼ì´ ì—†ì–´ ë³‘í•© í‚¤ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","        return d\n","\n","    df1 = ensure_shop_menu_columns(df1)\n","    df  = ensure_shop_menu_columns(df)\n","\n","    # 2) ê°€ì ¸ì˜¬ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬\n","    cols_available = [c for c in cols_to_add if c in df.columns]\n","    if not cols_available:\n","        raise KeyError(f\"dfì— ê°€ì ¸ì˜¬ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {cols_to_add}\")\n","\n","    # 3) dfì—ì„œ í‚¤ ì¤‘ë³µ ì œê±°(ìµœê·¼/ë§ˆì§€ë§‰ ê°’ ìš°ì„ )\n","    key_cols = [shop_col, menu_col, date_col]\n","    df_pull = (df[key_cols + cols_available]\n","               .sort_values(key_cols)\n","               .drop_duplicates(subset=key_cols, keep='last'))\n","\n","    # 4) ë¨¸ì§€\n","    suffix = \"_from_df\"\n","    merged = df1.merge(df_pull, on=key_cols, how='left',\n","                       suffixes=('', suffix))\n","\n","    # 5) ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê°’ ê²°í•©\n","    for c in cols_available:\n","        src_col = c + suffix\n","        if src_col not in merged.columns:\n","            continue\n","\n","        # ì •ìˆ˜/ë¶ˆë¦¬ì–¸ ì„±ê²© ì»¬ëŸ¼ì€ ìš°ì„  floatë¡œ ê²°í•© í›„ ìºìŠ¤íŒ…\n","        if prefer_df1:\n","            # df1 ê°’ì´ ìš°ì„ : df1 ê°’ì´ ê²°ì¸¡ì´ë©´ df ê°’ìœ¼ë¡œ ì±„ì›€\n","            merged[c] = merged[c].combine_first(merged[src_col])\n","        else:\n","            # df ê°’ì´ ìš°ì„ : df ê°’ì´ ìˆìœ¼ë©´ ë®ì–´ì”€\n","            merged[c] = merged[src_col].combine_first(merged[c])\n","\n","        # íƒ€ì… ì •ë¦¬(ê°€ëŠ¥í•˜ë©´ 0/1 ì •ìˆ˜ë¡œ)\n","        if pd.api.types.is_bool_dtype(merged[c]):\n","            merged[c] = merged[c].astype('int8')\n","        else:\n","            # ê°’ì´ 0/1 ë˜ëŠ” ê²°ì¸¡ì´ë¼ë©´ 0ìœ¼ë¡œ ì±„ìš°ê³  ì •ìˆ˜í™”\n","            if set(pd.unique(merged[c].dropna())) \u003c= {0,1}:\n","                merged[c] = merged[c].fillna(0).astype('int8')\n","\n","        merged.drop(columns=[src_col], inplace=True)\n","\n","    return merged\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‚¬ìš© ì˜ˆì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","df1 = add_off_and_zero_flags(\n","    df1, df,\n","    date_col='ì˜ì—…ì¼ì',\n","    shop_col='ì˜ì—…ì¥ëª…',\n","    menu_col='ë©”ë‰´ëª…',\n","    combined_col='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…',\n","    cols_to_add=('íœ´ë¬´ì¼','custom_offday','is_zero_sales_period'),\n","    prefer_df1=True   # df1 ê°’ ë³´ì¡´, ë¹ˆ ê³³ë§Œ dfë¡œ ë³´ì™„\n",")\n","\n","# ì ìš© í™•ì¸ (ê°„ë‹¨ ì ê²€)\n","for c in ['íœ´ë¬´ì¼','custom_offday','is_zero_sales_period']:\n","    if c in df1.columns:\n","        print(c, \"value counts -\u003e\")\n","        print(df1[c].value_counts(dropna=False).head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EiNuGBtXzB9r"},"outputs":[],"source":["# 'ì˜ì—…ì¥ëª…'ê³¼ 'ë©”ë‰´ëª…' ê²°í•©\n","df1['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = df1['ì˜ì—…ì¥ëª…'].astype(str) + '_' + df1['ë©”ë‰´ëª…'].astype(str)\n","df1['ì˜ì—…ì¼ì'] = pd.to_datetime(df1['ì˜ì—…ì¼ì'])  # ë¬¸ìì—´ â†’ datetime ë³€í™˜\n","df1['year'] = df1['ì˜ì—…ì¼ì'].dt.year\n","df1['month'] = df1['ì˜ì—…ì¼ì'].dt.month\n","df1['day'] = df1['ì˜ì—…ì¼ì'].dt.day\n","df1['weekday'] = pd.to_datetime(df1['ì˜ì—…ì¼ì']).dt.weekday\n","# 2. is_weekend ìƒì„± (ê¸ˆ~ì¼ = 5, 6)\n","df1['is_weekend'] = df1['weekday'].astype(int).isin([5, 6]).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ui2UpRClzKxW"},"outputs":[],"source":["import pandas as pd\n","import holidays\n","\n","# 1) ë‚ ì§œ ìº˜ë¦°ë” ìƒì„±\n","date_list = pd.date_range(start='2023-01-01', end='2024-06-15', freq='D')\n","\n","# 2) í•œêµ­ ê³µíœ´ì¼ ê°ì²´ (ì—°ë„ ì§€ì •í•´ë‘ë©´ ë¹ ë¦„/ì•ˆì „)\n","years = list(range(date_list.year.min(), date_list.year.max() + 1))\n","kr_holidays = holidays.KR(years=years)\n","\n","# 3) ìº˜ë¦°ë” DF + ê³µíœ´ì¼ í”Œë˜ê·¸/ì „í›„ì¼ í”Œë˜ê·¸\n","holiday_df = pd.DataFrame({'ds': date_list})\n","holiday_df['holiday'] = holiday_df['ds'].apply(lambda x: 'holiday' if x in kr_holidays else 'non-holiday')\n","holiday_df['is_holiday'] = (holiday_df['holiday'] == 'holiday').astype('int8')\n","\n","holiday_df = holiday_df.sort_values('ds')\n","holiday_df['holiday_prev1'] = holiday_df['is_holiday'].shift(1).fillna(0).astype('int8')\n","holiday_df['holiday_next1'] = holiday_df['is_holiday'].shift(-1).fillna(0).astype('int8')\n","\n","# 4) ì›ë³¸ df1ì— ë³‘í•©\n","df1 = df1.merge(\n","    holiday_df[['ds', 'holiday', 'is_holiday', 'holiday_prev1', 'holiday_next1']],\n","    how='left', left_on='ì˜ì—…ì¼ì', right_on='ds'\n",")\n","\n","# 5) ì •ë¦¬\n","df1.drop(columns=['ds'], inplace=True)\n","# (ì„ íƒ) ìˆ«ìí˜• ì¼ê´€ì„± ìœ ì§€\n","df1['holiday_prev1']  = pd.to_numeric(df1['holiday_prev1'], errors='coerce').fillna(0).astype('int8')\n","df1['holiday_next1']  = pd.to_numeric(df1['holiday_next1'], errors='coerce').fillna(0).astype('int8')\n","df1['is_holiday']     = pd.to_numeric(df1['is_holiday'],  errors='coerce').fillna(0).astype('int8')\n","# ë¬¸ìì—´ 'holiday'/'non-holiday' ìœ ì§€ê°€ í•„ìš” ì—†ìœ¼ë©´ ì•„ë˜ ì¤„ë¡œ ì‚­ì œ ê°€ëŠ¥\n","df1.drop(columns=['holiday'], inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RGhgCgJQ0QR3"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df1"},"text/html":["\n","  \u003cdiv id=\"df-5b812d1b-6c47-4cbd-b387-1b3a2c380b35\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eì˜ì—…ì¥ëª…\u003c/th\u003e\n","      \u003cth\u003eë©”ë‰´ëª…\u003c/th\u003e\n","      \u003cth\u003eì˜ì—…ì¼ì\u003c/th\u003e\n","      \u003cth\u003eë§¤ì¶œìˆ˜ëŸ‰\u003c/th\u003e\n","      \u003cth\u003eíœ´ë¬´ì¼\u003c/th\u003e\n","      \u003cth\u003ecustom_offday\u003c/th\u003e\n","      \u003cth\u003eis_zero_sales_period\u003c/th\u003e\n","      \u003cth\u003eì˜ì—…ì¥ëª…_ë©”ë‰´ëª…\u003c/th\u003e\n","      \u003cth\u003eyear\u003c/th\u003e\n","      \u003cth\u003emonth\u003c/th\u003e\n","      \u003cth\u003eday\u003c/th\u003e\n","      \u003cth\u003eweekday\u003c/th\u003e\n","      \u003cth\u003eis_weekend\u003c/th\u003e\n","      \u003cth\u003eis_holiday\u003c/th\u003e\n","      \u003cth\u003eholiday_prev1\u003c/th\u003e\n","      \u003cth\u003eholiday_next1\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ\u003c/td\u003e\n","      \u003ctd\u003e1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023-01-01\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e6\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ\u003c/td\u003e\n","      \u003ctd\u003e1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023-01-02\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ\u003c/td\u003e\n","      \u003ctd\u003e1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023-01-03\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ\u003c/td\u003e\n","      \u003ctd\u003e1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023-01-04\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ\u003c/td\u003e\n","      \u003ctd\u003e1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023-01-05\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸\u003c/td\u003e\n","      \u003ctd\u003e2023\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b812d1b-6c47-4cbd-b387-1b3a2c380b35')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-5b812d1b-6c47-4cbd-b387-1b3a2c380b35 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5b812d1b-6c47-4cbd-b387-1b3a2c380b35');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","    \u003cdiv id=\"df-c52a6546-b345-4c7b-99c6-a6795bec78ae\"\u003e\n","      \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-c52a6546-b345-4c7b-99c6-a6795bec78ae')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","      \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() =\u003e {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-c52a6546-b345-4c7b-99c6-a6795bec78ae button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["         ì˜ì—…ì¥ëª…      ë©”ë‰´ëª…       ì˜ì—…ì¼ì  ë§¤ì¶œìˆ˜ëŸ‰  íœ´ë¬´ì¼  custom_offday  \\\n","0  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2023-01-01   0.0    0              0   \n","1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2023-01-02   0.0    0              1   \n","2  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2023-01-03   0.0    0              0   \n","3  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2023-01-04   0.0    0              0   \n","4  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2023-01-05   0.0    0              0   \n","\n","   is_zero_sales_period            ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…  year  month  day  weekday  \\\n","0                     1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸  2023      1    1        6   \n","1                     1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸  2023      1    2        0   \n","2                     1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸  2023      1    3        1   \n","3                     1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸  2023      1    4        2   \n","4                     1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸  2023      1    5        3   \n","\n","   is_weekend  is_holiday  holiday_prev1  holiday_next1  \n","0           1           1              0              0  \n","1           0           0              1              0  \n","2           0           0              0              0  \n","3           0           0              0              0  \n","4           0           0              0              0  "]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["df1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EklsXyHF1l3V"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Œ ëª¨ë“  ì¡°í•©ì—ì„œ ë§¤ì¶œì´ 0ì¸ ì˜ì—…ì¼ì ìˆ˜: 17ì¼\n","[Timestamp('2023-03-02 00:00:00'), Timestamp('2023-03-03 00:00:00'), Timestamp('2023-03-04 00:00:00'), Timestamp('2023-03-05 00:00:00'), Timestamp('2023-03-06 00:00:00')]\n"]}],"source":["# ê° ë‚ ì§œë³„ ì „ì²´ ë§¤ì¶œ í•©ì‚°\n","total_sales_per_day = df1.groupby('ì˜ì—…ì¼ì')['ë§¤ì¶œìˆ˜ëŸ‰'].sum().reset_index()\n","\n","# ë§¤ì¶œìˆ˜ëŸ‰ì´ 0ì¸ ì˜ì—…ì¼ì í•„í„°ë§\n","zero_sales_dates = total_sales_per_day[total_sales_per_day['ë§¤ì¶œìˆ˜ëŸ‰'] == 0]['ì˜ì—…ì¼ì']\n","\n","print(f\"ğŸ“Œ ëª¨ë“  ì¡°í•©ì—ì„œ ë§¤ì¶œì´ 0ì¸ ì˜ì—…ì¼ì ìˆ˜: {len(zero_sales_dates)}ì¼\")\n","print(zero_sales_dates.tolist()[:5])  # ì¼ë¶€ ì¶œë ¥"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CEp0MDa03bMu"},"outputs":[],"source":["#ì˜ì—…ì¥ë³„ ë§¤ì¶œ ìˆ˜ëŸ‰ì´ ìƒìœ„ê¶Œì¸ ë©”ë‰´ í‘œì‹œ\n","# âŠ ì—…ì¥Â·ë©”ë‰´ë³„ ëˆ„ì  ë§¤ì¶œ ê³„ì‚°\n","menu_sales = (\n","    df1\n","    .groupby(['ì˜ì—…ì¥ëª…', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'])['ë§¤ì¶œìˆ˜ëŸ‰']\n","    .sum()\n","    .reset_index()\n","    .rename(columns={'ë§¤ì¶œìˆ˜ëŸ‰': 'total_menu_sales'})\n",")\n","\n","main_menu_flags = []\n","\n","# â‹ ì—…ì¥ë³„ë¡œ ì£¼ìš” ë©”ë‰´ ì‹ë³„ (ë¹„ìœ¨ 0.4, ë˜ëŠ” ìµœê³  ë§¤ì¶œì˜ 0.5ë°° ë¯¸ë§Œ ê¸°ì¤€)\n","for store, grp in menu_sales.groupby('ì˜ì—…ì¥ëª…'):\n","    grp = grp.sort_values('total_menu_sales', ascending=False).reset_index(drop=True)\n","    sales = grp['total_menu_sales'].values\n","    max_sales = sales[0]\n","\n","    # consecutive ë¹„ìœ¨ ë³€í™” ê³„ì‚° (sales[i] / sales[i-1])\n","    ratios = sales[1:] / (sales[:-1] + 1e-6)\n","\n","    # â‘  ë¹„ìœ¨ì´ 0.4 ë¯¸ë§Œì¸ ìµœì´ˆ ìœ„ì¹˜\n","    idx_ratio = np.where(ratios \u003c 0.4)[0]\n","    cutoff_ratio = idx_ratio[0] + 1 if len(idx_ratio) \u003e 0 else len(sales)\n","\n","    # â‘¡ ìµœê³  ë§¤ì¶œì˜ 0.5ë°° ë¯¸ë§Œì´ ë˜ëŠ” ìµœì´ˆ ìœ„ì¹˜\n","    idx_half = np.where(sales \u003c 0.5 * max_sales)[0]\n","    cutoff_half = idx_half[0] if len(idx_half) \u003e 0 else len(sales)\n","\n","    # ìµœì¢… ì»·ì˜¤í”„ëŠ” ë‘ ê¸°ì¤€ ì¤‘ ë” ì‘ì€ ì¸ë±ìŠ¤\n","    cutoff = min(cutoff_ratio, cutoff_half)\n","\n","    main_menus = grp.loc[:cutoff-1, 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].tolist()\n","\n","    for menu in grp['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']:\n","        main_menu_flags.append({\n","            'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': menu,\n","            'is_main_menu': int(menu in main_menus)\n","        })\n","\n","main_menu_df = pd.DataFrame(main_menu_flags)\n","\n","# âŒ df1ì— ë³‘í•©\n","df1= df1.merge(\n","    main_menu_df,\n","    on='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…',\n","    how='left'\n",")\n","\n","# â NaN ì²˜ë¦¬ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ)\n","df1['is_main_menu'] = df1['is_main_menu'].fillna(0).astype(int)"]},{"cell_type":"markdown","metadata":{"id":"-BDZWulV5k4V"},"source":["ì‹œê³„ì—´ ë°ì´í„° ì´ˆë°˜ì— 60ì¼ ì´ìƒ ì—°ì† 0 ë°ì´í„°ì¼ ì‹œ ì‹ ê·œ ì¶œì‹œë¼ê³  ê°€ì •"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sX7NOasE4swK"},"outputs":[],"source":["import pandas as pd\n","\n","# 1) ë©”ë‰´ë³„ ì²« íŒë§¤ì¼\n","first_sale = (df1[df1['ë§¤ì¶œìˆ˜ëŸ‰'] \u003e 0]\n","              .groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ì˜ì—…ì¼ì']\n","              .min()\n","              .rename('first_sale_date'))\n","\n","# 2) ë©”ë‰´ë³„ ì²« ê´€ì¸¡ì¼\n","first_seen = (df1.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ì˜ì—…ì¼ì']\n","                .min()\n","                .rename('first_seen_date'))\n","\n","# 3) ë§¤ì¥ë³„(ì „ì²´ ë©”ë‰´ í•©) ì¼ë³„ ë§¤ì¶œ\n","store_daily = (df1.groupby(['ì˜ì—…ì¥ëª…','ì˜ì—…ì¼ì'])['ë§¤ì¶œìˆ˜ëŸ‰']\n","                 .sum()\n","                 .reset_index()\n","                 .rename(columns={'ë§¤ì¶œìˆ˜ëŸ‰':'store_sales'}))\n","\n","# 4) ë©”ë‰´â€“ë§¤ì¥ ë§¤í•‘\n","menu_store = df1[['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ì—…ì¥ëª…']].drop_duplicates()\n","\n","# 5) ë©”ë‰´ë³„ ì§€í‘œ ì§‘ê³„\n","info = (menu_store\n","        .merge(first_sale, left_on='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', right_index=True, how='left')\n","        .merge(first_seen, left_on='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', right_index=True, how='left'))\n","\n","# ì¶œì‹œ ì „ ë§¤ì¥ ê°€ë™ ì—¬ë¶€(í•´ë‹¹ ë§¤ì¥ì—ì„œ ì²«íŒë§¤ ì´ì „ì— ë‹¤ë¥¸ ë§¤ì¶œ \u003e 0ì´ ìˆì—ˆëŠ”ì§€)\n","def had_store_sales_before(row):\n","    if pd.isna(row['first_sale_date']):\n","        # ì•„ì˜ˆ íŒë§¤ê°€ ì—†ì—ˆë˜ ë©”ë‰´: ì‹ ê·œ íŒë‹¨ ë¶ˆê°€\n","        return False\n","    s = store_daily[(store_daily['ì˜ì—…ì¥ëª…']==row['ì˜ì—…ì¥ëª…']) \u0026\n","                    (store_daily['ì˜ì—…ì¼ì'] \u003c row['first_sale_date']) \u0026\n","                    (store_daily['store_sales'] \u003e 0)]\n","    return len(s) \u003e 0\n","\n","info['store_active_before'] = info.apply(had_store_sales_before, axis=1)\n","\n","# ì¶œì‹œ ì „ ê³µë°± ì¼ìˆ˜\n","info['days_idle_before'] = (info['first_sale_date'] - info['first_seen_date']).dt.days\n","\n","# ì¶œì‹œ í›„ ì´ˆê¸° 60ì¼ ë™ì•ˆì˜ ë¹„0 íŒë§¤ì¼ ìˆ˜\n","k_window = 60\n","k_min_nonzero = 5\n","\n","def count_nonzero_after(menu):\n","    g = df1[df1['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']==menu].sort_values('ì˜ì—…ì¼ì')\n","    fs = info.loc[info['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']==menu, 'first_sale_date'].values[0]\n","    if pd.isna(fs):\n","        return 0\n","    sub = g[(g['ì˜ì—…ì¼ì']\u003e=fs) \u0026 (g['ì˜ì—…ì¼ì']\u003cfs + pd.Timedelta(days=k_window))]\n","    return int((sub['ë§¤ì¶œìˆ˜ëŸ‰'] \u003e 0).sum())\n","\n","nz_after = {m: count_nonzero_after(m) for m in info['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']}\n","info['nonzero_days_60d'] = info['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].map(nz_after)\n","\n","# ìµœì¢…: ì‹ ê·œ ì¶œì‹œ ì¶”ì •\n","info['is_probable_launch'] = (\n","    info['first_sale_date'].notna() \u0026\n","    info['store_active_before'] \u0026\n","    (info['days_idle_before'] \u003e= 60) \u0026\n","    (info['nonzero_days_60d'] \u003e= k_min_nonzero)\n",").astype(int)\n","\n","# ê²°ê³¼ í™•ì¸\n","launch_candidates = info[info['is_probable_launch']==1] \\\n","    .sort_values(['ì˜ì—…ì¥ëª…','first_sale_date'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uXHNo-bH5jeX"},"outputs":[],"source":["# info: (ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…, ì˜ì—…ì¥ëª…, first_sale_date, first_seen_date, store_active_before, days_idle_before, nonzero_days_60d, is_probable_launch)\n","\n","# â‘  dfì— ì¶œì‹œ ë©”íƒ€ ë³‘í•©\n","df1 = df1.merge(\n","    info[['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','first_sale_date','first_seen_date','is_probable_launch']],\n","    on='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', how='left'\n",")\n","\n","# â‘¡ ì¶œì‹œ ì „/í›„ êµ¬ë¶„ ë° ê²½ê³¼ì¼\n","df1['days_since_launch'] = (df1['ì˜ì—…ì¼ì'] - df1['first_sale_date']).dt.days\n","df1['is_prelaunch']      = df1['days_since_launch'].lt(0).astype(int)      # ì¶œì‹œ ì „(True=1)\n","df1['is_postlaunch']     = df1['days_since_launch'].ge(0).astype(int)      # ì¶œì‹œ í›„(True=1)\n","\n","# â‘¢ ì¶œì‹œ ì§í›„ ì½œë“œìŠ¤íƒ€íŠ¸ êµ¬ê°„(ì˜ˆ: 60ì¼) í‘œì‹œ\n","COLD_DAYS = 60\n","df1['is_coldstart_60d'] = ((df1['days_since_launch'] \u003e= 0) \u0026\n","                          (df1['days_since_launch'] \u003c COLD_DAYS)).astype(int)\n","\n","# â‘¤ ì¶œì‹œ ì „ êµ¬ì¡°ì  0ì— ëŒ€í•œ ëª…í™•í•œ ë§ˆìŠ¤í¬(í•™ìŠµ ì œì™¸ìš©)\n","df1['structural_zero'] = ((df1['is_prelaunch'] == 1) \u0026 (df1['ë§¤ì¶œìˆ˜ëŸ‰'] == 0)).astype(int)\n","\n","# â‘¥ íŒë§¤ ì‹œì‘ì´ í•œ ë²ˆë„ ì—†ë˜ ë©”ë‰´(NaT) ì²˜ë¦¬ìš© í”Œë˜ê·¸\n","df1['never_sold'] = df1['first_sale_date'].isna().astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RqQ3CCvYYsF4"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 102676 entries, 0 to 102675\n","Data columns (total 26 columns):\n"," #   Column                Non-Null Count   Dtype         \n","---  ------                --------------   -----         \n"," 0   ì˜ì—…ì¥ëª…                  102676 non-null  object        \n"," 1   ë©”ë‰´ëª…                   102676 non-null  object        \n"," 2   ì˜ì—…ì¼ì                  102676 non-null  datetime64[ns]\n"," 3   ë§¤ì¶œìˆ˜ëŸ‰                  102676 non-null  float64       \n"," 4   íœ´ë¬´ì¼                   102676 non-null  int8          \n"," 5   custom_offday         102676 non-null  int8          \n"," 6   is_zero_sales_period  102676 non-null  int64         \n"," 7   ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…              102676 non-null  object        \n"," 8   year                  102676 non-null  int32         \n"," 9   month                 102676 non-null  int32         \n"," 10  day                   102676 non-null  int32         \n"," 11  weekday               102676 non-null  int32         \n"," 12  is_weekend            102676 non-null  int64         \n"," 13  is_holiday            102676 non-null  int8          \n"," 14  holiday_prev1         102676 non-null  int8          \n"," 15  holiday_next1         102676 non-null  int8          \n"," 16  is_main_menu          102676 non-null  int64         \n"," 17  first_sale_date       102676 non-null  datetime64[ns]\n"," 18  first_seen_date       102676 non-null  datetime64[ns]\n"," 19  is_probable_launch    102676 non-null  int64         \n"," 20  days_since_launch     102676 non-null  int64         \n"," 21  is_prelaunch          102676 non-null  int64         \n"," 22  is_postlaunch         102676 non-null  int64         \n"," 23  is_coldstart_60d      102676 non-null  int64         \n"," 24  structural_zero       102676 non-null  int64         \n"," 25  never_sold            102676 non-null  int64         \n","dtypes: datetime64[ns](3), float64(1), int32(4), int64(10), int8(5), object(3)\n","memory usage: 15.4+ MB\n"]}],"source":["df1.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"l9ZeDsamoxX6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def add_menu_month_features(\n","    df: pd.DataFrame,\n","    date_col=\"ì˜ì—…ì¼ì\",\n","    shop_col=\"ì˜ì—…ì¥ëª…\",\n","    menu_col=\"ë©”ë‰´ëª…\",\n","    combined_col=\"ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…\",\n","    sales_col=\"ë§¤ì¶œìˆ˜ëŸ‰\",\n","    # ì„ê³„/íŒŒë¼ë¯¸í„°\n","    top_k=3, bottom_k=3,          # MOY ìƒ/í•˜ìœ„ ê°œìˆ˜\n","    good_month_quantile=0.75,     # â€œì¢‹ì€ ë‹¬â€ ì „ì›” ë¡¤ë§ ë¶„ìœ„\n","    rolling_q_window=12,          # ë¶„ìœ„ ë¡¤ë§ ìœˆë„ìš°(ê°œì›”)\n","    sharp_drop_pct=-0.40,         # ì „ì›” vs ì „ì „ì›” ê¸‰ë½ ì„ê³„(%)\n","    min_abs_drop=20,              # ê¸‰ë½ ì ˆëŒ€ëŸ‰ ì„ê³„\n","    min_prev=30,                  # ì „ì „ì›” ìµœì†Œ ê·œëª¨\n","    share_threshold=0.05,         # ë‚®ì€ ê¸°ì—¬ ì„ê³„ (ì „ì›” ëˆ„ì ì ìœ ìœ¨)\n","    min_total_sales=1,            # ì „ì›”ê¹Œì§€ ëˆ„ì í•© ìµœì†Œ\n","    min_obs_moy=2                 # MOY í‰ê·  ì‚°ì¶œ ìµœì†Œ ê´€ì¸¡ìˆ˜(ì „ì›” ì‹œì ê¹Œì§€)\n","):\n","    \"\"\"\n","    âœ… ëˆ„ìˆ˜ ë°©ì§€ ë²„ì „: 'ì „ì›” ê¸°ì¤€(lag1)'ìœ¼ë¡œë§Œ ì›” ì§€í‘œë¥¼ ë§Œë“¤ì–´ ì¼(í–‰) ë‹¨ìœ„ë¡œ ë¶™ì…ë‹ˆë‹¤.\n","\n","    ìƒì„± ì»¬ëŸ¼(ëª¨ë‘ lag1, ì „ì›” ê¸°ì¤€):\n","      - lag1_is_high_season_moy, lag1_is_low_season_moy\n","      - lag1_is_good_month_quantile, lag1_is_sharp_drop_month\n","      - lag1_is_low_share_month, lag1_month_share\n","      - lag1_monthly_sales (ë³´ì¡° ì§€í‘œ: ì „ì›” ì›”í•©)\n","    \"\"\"\n","    out = df.copy()\n","    out[date_col] = pd.to_datetime(out[date_col])\n","\n","    # í‚¤ í™•ë³´\n","    if shop_col not in out.columns or menu_col not in out.columns:\n","        if combined_col in out.columns:\n","            pair = out[combined_col].astype(str).str.split('_', n=1, expand=True)\n","            out[shop_col] = pair[0]\n","            out[menu_col] = pair[1] if pair.shape[1] \u003e 1 else ''\n","        else:\n","            raise KeyError(\"ì˜ì—…ì¥/ë©”ë‰´ í‚¤ê°€ ì—†ì–´ í”¼ì²˜ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","    # ì›” period\n","    if ('ì›”_ê¸°ê°„' not in out.columns) or (not isinstance(out['ì›”_ê¸°ê°„'].dtype, pd.PeriodDtype)):\n","        out['ì›”_ê¸°ê°„'] = out[date_col].dt.to_period('M')\n","\n","    key_pair = [shop_col, menu_col]\n","    key = key_pair + ['ì›”_ê¸°ê°„']\n","\n","    # â”€â”€ 1) ë©”ë‰´Ã—ì›” ì›”í•© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    monthly = (\n","        out.groupby(key, as_index=False)[sales_col]\n","           .sum()\n","           .rename(columns={sales_col: 'monthly_sales'})\n","           .sort_values(key)\n","    )\n","    monthly['month']   = monthly['ì›”_ê¸°ê°„'].dt.month\n","    monthly['m_prev']  = monthly.groupby(key_pair)['monthly_sales'].shift(1)\n","    monthly['m_prev2'] = monthly.groupby(key_pair)['monthly_sales'].shift(2)\n","    monthly['cum_prev'] = monthly.groupby(key_pair)['monthly_sales'].cumsum().shift(1)\n","\n","    # â”€â”€ 2) ì „ì›” ëˆ„ì  ì ìœ ìœ¨ \u0026 ë‚®ì€ ê¸°ì—¬ë‹¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    monthly['lag1_month_share'] = (monthly['m_prev'] / monthly['cum_prev'])\n","    monthly['lag1_month_share'] = monthly['lag1_month_share'] \\\n","        .replace([np.inf, -np.inf], np.nan).fillna(0.0).astype('float32')\n","\n","    monthly['lag1_is_low_share_month'] = (\n","        (monthly['cum_prev'] \u003e= float(min_total_sales)) \u0026\n","        (monthly['lag1_month_share'] \u003c= float(share_threshold))\n","    ).astype('int8')\n","\n","    # â”€â”€ 3) ì „ì›” â€œì¢‹ì€ ë‹¬â€: ì „ì›”ê¹Œì§€ ë¡¤ë§ ë¶„ìœ„ì™€ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    monthly['q_thr_prev'] = monthly.groupby(key_pair)['monthly_sales'] \\\n","        .transform(lambda s: s.shift(1).rolling(rolling_q_window, min_periods=3)\n","                   .quantile(good_month_quantile))\n","    monthly['lag1_is_good_month_quantile'] = (\n","        (monthly['m_prev'] \u003e= monthly['q_thr_prev'])\n","    ).fillna(0).astype('int8')\n","\n","    # â”€â”€ 4) ì „ì›” ê¸‰ë½(ì „ì›” vs ì „ì „ì›”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        pct = (monthly['m_prev'] - monthly['m_prev2']) / monthly['m_prev2']\n","    abs_change = (monthly['m_prev'] - monthly['m_prev2'])\n","    cond = monthly['m_prev2'].notna() \u0026 (monthly['m_prev2'] \u003e= min_prev)\n","    monthly['lag1_is_sharp_drop_month'] = (\n","        cond \u0026 (pct \u003c= sharp_drop_pct) \u0026 (abs_change.abs() \u003e= min_abs_drop)\n","    ).astype('int8')\n","\n","    # â”€â”€ 5) ì „ì›” MOY ì‹œì¦Œì„±(ì „ì›” ì‹œì ê¹Œì§€ì˜ MOY ëˆ„ì í‰ê· ìœ¼ë¡œ ìˆœìœ„í™”) â”€â”€â”€\n","    #   ê° ì›”(1..12)ì— ëŒ€í•´ ëˆ„ì í•©/ëˆ„ì ê±´ìˆ˜ â†’ ì „ì›” ì‹œì  í‰ê· \n","    for mo in range(1, 13):\n","        mask = (monthly['month'] == mo).astype('int8')\n","        monthly[f'cum_sum_m{mo}'] = (monthly['monthly_sales'] * mask).cumsum().shift(1)\n","        monthly[f'cum_cnt_m{mo}'] = (mask).cumsum().shift(1)\n","\n","    monthly['month_prev'] = monthly['month'].shift(1)\n","\n","    def _rank_moy_row(row):\n","        means = []\n","        for mo in range(1, 13):\n","            s = row.get(f'cum_sum_m{mo}', np.nan)\n","            c = row.get(f'cum_cnt_m{mo}', np.nan)\n","            means.append(np.nan if (pd.isna(c) or c \u003c min_obs_moy) else (s / c if c else np.nan))\n","        means = np.array(means, dtype='float64')\n","        valid = ~np.isnan(means)\n","        if pd.isna(row['month_prev']) or not valid.any():\n","            return 0, 0\n","        idx_prev = int(row['month_prev']) - 1\n","        if not valid[idx_prev]:\n","            return 0, 0\n","        k_high = min(top_k, valid.sum())\n","        k_low  = min(bottom_k, valid.sum())\n","        # ìƒìœ„ k\n","        vals_high = np.where(valid, means, -np.inf)\n","        top_idx   = np.argsort(vals_high)[-k_high:] if k_high \u003e 0 else np.array([], dtype=int)\n","        high_flag = int(idx_prev in top_idx)\n","        # í•˜ìœ„ k\n","        vals_low  = np.where(valid, means, np.inf)\n","        bot_idx   = np.argsort(vals_low)[:k_low] if k_low \u003e 0 else np.array([], dtype=int)\n","        low_flag  = int(idx_prev in bot_idx)\n","        return high_flag, low_flag\n","\n","    high_low = monthly.apply(lambda r: _rank_moy_row(r), axis=1, result_type='expand')\n","    monthly['lag1_is_high_season_moy'] = high_low.iloc[:, 0].astype('int8')\n","    monthly['lag1_is_low_season_moy']  = high_low.iloc[:, 1].astype('int8')\n","\n","    # ë³´ì¡°: ì „ì›” ì›”í•©\n","    monthly['lag1_monthly_sales'] = monthly['m_prev'].fillna(0).astype('float32')\n","\n","    # â”€â”€ 6) ì¼(í–‰) ë‹¨ìœ„ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    attach = [\n","        'lag1_month_share',\n","        'lag1_is_low_share_month',\n","        'lag1_is_good_month_quantile',\n","        'lag1_is_sharp_drop_month',\n","        'lag1_is_high_season_moy',\n","        'lag1_is_low_season_moy',\n","        'lag1_monthly_sales'\n","    ]\n","    out = out.merge(monthly[key + attach], on=key, how='left')\n","\n","    # ê²°ì¸¡/íƒ€ì… ë§ˆë¬´ë¦¬\n","    for c in attach:\n","        if c.endswith('month_share') or c.endswith('monthly_sales'):\n","            out[c] = out[c].fillna(0.0).astype('float32')\n","        else:\n","            out[c] = out[c].fillna(0).astype('int8')\n","\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"r8LrL4aZo0Eu"},"outputs":[],"source":["df1 = add_menu_month_features(\n","    df1,\n","    date_col=\"ì˜ì—…ì¼ì\",\n","    shop_col=\"ì˜ì—…ì¥ëª…\",\n","    menu_col=\"ë©”ë‰´ëª…\",\n","    sales_col=\"ë§¤ì¶œìˆ˜ëŸ‰\",\n","    top_k=3, bottom_k=3,\n","    good_month_quantile=0.75,\n","    rolling_q_window=12,\n","    sharp_drop_pct=-0.40,\n","    min_abs_drop=20,\n","    min_prev=30,\n","    share_threshold=0.05\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MQCl8kDbaKRO"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 102676 entries, 0 to 102675\n","Data columns (total 34 columns):\n"," #   Column                       Non-Null Count   Dtype         \n","---  ------                       --------------   -----         \n"," 0   ì˜ì—…ì¥ëª…                         102676 non-null  object        \n"," 1   ë©”ë‰´ëª…                          102676 non-null  object        \n"," 2   ì˜ì—…ì¼ì                         102676 non-null  datetime64[ns]\n"," 3   ë§¤ì¶œìˆ˜ëŸ‰                         102676 non-null  float64       \n"," 4   íœ´ë¬´ì¼                          102676 non-null  int8          \n"," 5   custom_offday                102676 non-null  int8          \n"," 6   is_zero_sales_period         102676 non-null  int64         \n"," 7   ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…                     102676 non-null  object        \n"," 8   year                         102676 non-null  int32         \n"," 9   month                        102676 non-null  int32         \n"," 10  day                          102676 non-null  int32         \n"," 11  weekday                      102676 non-null  int32         \n"," 12  is_weekend                   102676 non-null  int64         \n"," 13  is_holiday                   102676 non-null  int8          \n"," 14  holiday_prev1                102676 non-null  int8          \n"," 15  holiday_next1                102676 non-null  int8          \n"," 16  is_main_menu                 102676 non-null  int64         \n"," 17  first_sale_date              102676 non-null  datetime64[ns]\n"," 18  first_seen_date              102676 non-null  datetime64[ns]\n"," 19  is_probable_launch           102676 non-null  int64         \n"," 20  days_since_launch            102676 non-null  int64         \n"," 21  is_prelaunch                 102676 non-null  int64         \n"," 22  is_postlaunch                102676 non-null  int64         \n"," 23  is_coldstart_60d             102676 non-null  int64         \n"," 24  structural_zero              102676 non-null  int64         \n"," 25  never_sold                   102676 non-null  int64         \n"," 26  ì›”_ê¸°ê°„                         102676 non-null  period[M]     \n"," 27  lag1_month_share             102676 non-null  float32       \n"," 28  lag1_is_low_share_month      102676 non-null  int8          \n"," 29  lag1_is_good_month_quantile  102676 non-null  int8          \n"," 30  lag1_is_sharp_drop_month     102676 non-null  int8          \n"," 31  lag1_is_high_season_moy      102676 non-null  int8          \n"," 32  lag1_is_low_season_moy       102676 non-null  int8          \n"," 33  lag1_monthly_sales           102676 non-null  float32       \n","dtypes: datetime64[ns](3), float32(2), float64(1), int32(4), int64(10), int8(10), object(3), period[M](1)\n","memory usage: 17.4+ MB\n"]}],"source":["df1.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L9wb45urqOpz"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def add_low_share_month_feature_leak_safe(\n","    df: pd.DataFrame,\n","    *,\n","    date_col: str = 'ì˜ì—…ì¼ì',\n","    shop_col: str = 'ì˜ì—…ì¥ëª…',\n","    menu_col: str = 'ë©”ë‰´ëª…',\n","    combined_col: str = 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…',\n","    sales_col: str = 'ë§¤ì¶œìˆ˜ëŸ‰',\n","    period_col: str = 'ì›”_ê¸°ê°„',\n","    share_threshold: float = 0.05,          # ì„ê³„: â‰¤ 5%\n","    min_total_sales: float | int = 1,       # ëˆ„ì í•© ìµœì†Œ\n","    overwrite: bool = True                  # ë™ì¼ ì»¬ëŸ¼ ì¡´ì¬ ì‹œ ë®ì–´ì“¸ì§€\n",") -\u003e pd.DataFrame:\n","    \"\"\"\n","    í•™ìŠµ ì•ˆì „(ëˆ„ìˆ˜ ë°©ì§€) ë²„ì „:\n","      - lag1_month_share = (ì „ì›” ë§¤ì¶œ / ì „ì›”ê¹Œì§€ì˜ ëˆ„ì í•©)\n","      - lag1_is_low_share_month = (lag1_month_share â‰¤ ì„ê³„) \u0026 (ëˆ„ì í•© â‰¥ min_total_sales)\n","    ì¼(í–‰) ë‹¨ìœ„ DFì— ì›” í‚¤ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    out = df.copy()\n","    out[date_col] = pd.to_datetime(out[date_col])\n","\n","    # (ì˜ì—…ì¥, ë©”ë‰´) í™•ë³´\n","    if (shop_col not in out.columns) or (menu_col not in out.columns):\n","        if combined_col in out.columns:\n","            sp = out[combined_col].astype(str).str.split('_', n=1, expand=True)\n","            out[shop_col] = sp[0]\n","            out[menu_col] = sp[1] if sp.shape[1] \u003e 1 else ''\n","        else:\n","            raise KeyError(\"ì˜ì—…ì¥/ë©”ë‰´ í‚¤ê°€ ì—†ì–´ í”¼ì²˜ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","    # ì›” period\n","    if (period_col not in out.columns) or (not pd.api.types.is_period_dtype(out[period_col])):\n","        out[period_col] = out[date_col].dt.to_period('M')\n","\n","    key_pair = [shop_col, menu_col]\n","    key = key_pair + [period_col]\n","\n","    # (ì˜ì—…ì¥,ë©”ë‰´,ì›”) ì´ë§¤ì¶œ\n","    monthly = (\n","        out.groupby(key, as_index=False)[sales_col]\n","           .sum()\n","           .rename(columns={sales_col: 'monthly_sales'})\n","           .sort_values(key)\n","    )\n","\n","    # ì „ì›”/ì „ì›”ê¹Œì§€ ëˆ„ì í•©\n","    monthly['m_prev']   = monthly.groupby(key_pair)['monthly_sales'].shift(1)\n","    monthly['cum_prev'] = monthly.groupby(key_pair)['monthly_sales'].cumsum().shift(1)\n","\n","    # ì „ì›” ì ìœ ìœ¨ \u0026 ë‚®ì€ ê¸°ì—¬ í”Œë˜ê·¸\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        lag_share = monthly['m_prev'] / monthly['cum_prev']\n","    monthly['lag1_month_share'] = (\n","        pd.Series(lag_share).replace([np.inf, -np.inf], np.nan).fillna(0.0).astype('float32')\n","    )\n","    monthly['lag1_is_low_share_month'] = (\n","        (monthly['cum_prev'] \u003e= float(min_total_sales)) \u0026\n","        (monthly['lag1_month_share'] \u003c= float(share_threshold))\n","    ).astype('int8')\n","\n","    # ì¼(í–‰) ë‹¨ìœ„ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸\n","    attach = ['lag1_month_share', 'lag1_is_low_share_month']\n","    merged = out.merge(monthly[key + attach], on=key, how='left', suffixes=('', '_calc'))\n","\n","    # ë®ì–´ì“°ê¸°/ê²°ì¸¡ ì²˜ë¦¬\n","    for col, dtype, fill in [('lag1_month_share', 'float32', 0.0),\n","                             ('lag1_is_low_share_month', 'int8', 0)]:\n","        src = col + '_calc'\n","        if src in merged.columns:\n","            if overwrite or col not in merged.columns:\n","                merged[col] = merged[src]\n","            else:\n","                merged[col] = merged[col].combine_first(merged[src])\n","            merged.drop(columns=[src], inplace=True)\n","        merged[col] = merged[col].fillna(fill).astype(dtype)\n","\n","    return merged\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ul2aTQOdhNlB"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-2447785421.py:36: DeprecationWarning: is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead\n","  if (period_col not in out.columns) or (not pd.api.types.is_period_dtype(out[period_col])):\n"]}],"source":["df1 = add_low_share_month_feature_leak_safe(\n","    df1,\n","    share_threshold=0.05,\n","    min_total_sales=1,\n","    overwrite=True\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ICqjHEyZf620"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 102676 entries, 0 to 102675\n","Data columns (total 34 columns):\n"," #   Column                       Non-Null Count   Dtype         \n","---  ------                       --------------   -----         \n"," 0   ì˜ì—…ì¥ëª…                         102676 non-null  object        \n"," 1   ë©”ë‰´ëª…                          102676 non-null  object        \n"," 2   ì˜ì—…ì¼ì                         102676 non-null  datetime64[ns]\n"," 3   ë§¤ì¶œìˆ˜ëŸ‰                         102676 non-null  float64       \n"," 4   íœ´ë¬´ì¼                          102676 non-null  int8          \n"," 5   custom_offday                102676 non-null  int8          \n"," 6   is_zero_sales_period         102676 non-null  int64         \n"," 7   ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…                     102676 non-null  object        \n"," 8   year                         102676 non-null  int32         \n"," 9   month                        102676 non-null  int32         \n"," 10  day                          102676 non-null  int32         \n"," 11  weekday                      102676 non-null  int32         \n"," 12  is_weekend                   102676 non-null  int64         \n"," 13  is_holiday                   102676 non-null  int8          \n"," 14  holiday_prev1                102676 non-null  int8          \n"," 15  holiday_next1                102676 non-null  int8          \n"," 16  is_main_menu                 102676 non-null  int64         \n"," 17  first_sale_date              102676 non-null  datetime64[ns]\n"," 18  first_seen_date              102676 non-null  datetime64[ns]\n"," 19  is_probable_launch           102676 non-null  int64         \n"," 20  days_since_launch            102676 non-null  int64         \n"," 21  is_prelaunch                 102676 non-null  int64         \n"," 22  is_postlaunch                102676 non-null  int64         \n"," 23  is_coldstart_60d             102676 non-null  int64         \n"," 24  structural_zero              102676 non-null  int64         \n"," 25  never_sold                   102676 non-null  int64         \n"," 26  ì›”_ê¸°ê°„                         102676 non-null  period[M]     \n"," 27  lag1_month_share             102676 non-null  float32       \n"," 28  lag1_is_low_share_month      102676 non-null  int8          \n"," 29  lag1_is_good_month_quantile  102676 non-null  int8          \n"," 30  lag1_is_sharp_drop_month     102676 non-null  int8          \n"," 31  lag1_is_high_season_moy      102676 non-null  int8          \n"," 32  lag1_is_low_season_moy       102676 non-null  int8          \n"," 33  lag1_monthly_sales           102676 non-null  float32       \n","dtypes: datetime64[ns](3), float32(2), float64(1), int32(4), int64(10), int8(10), object(3), period[M](1)\n","memory usage: 17.4+ MB\n"]}],"source":["df1.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GirW1a4Z56Wq"},"outputs":[],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","LOOKBACK = 28\n","HORIZON  = 7\n","BATCH_SIZE = 64\n","EPOCHS = 10\n","LR = 1e-3\n","EMBED_DIM = 8        # ë©”ë‰´ ì„ë² ë”© ì°¨ì›\n","HIDDEN = 64\n","NUM_LAYERS = 1\n","DROPOUT = 0.1\n","\n","# ==== 1) ìœ í‹¸: ì†ì‹¤/ì§€í‘œ ====\n","class SMAPE_MAE_Loss(nn.Module):\n","    def __init__(self, alpha=0.8):\n","        super().__init__()\n","        self.alpha = alpha\n","    def forward(self, pred, target):\n","        smape = torch.mean(2 * torch.abs(pred - target) / (torch.abs(pred) + torch.abs(target) + 1e-8))\n","        mae = torch.mean(torch.abs(pred - target))\n","        return self.alpha * smape + (1 - self.alpha) * mae\n","\n","@torch.no_grad()\n","def smape_mae(pred, target):\n","    smape = torch.mean(2 * torch.abs(pred - target) / (torch.abs(pred) + torch.abs(target) + 1e-8)).item()\n","    mae = torch.mean(torch.abs(pred - target)).item()\n","    return smape, mae"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3Br78G947yOj"},"outputs":[],"source":["# ==== 2) ë°ì´í„°ì…‹ ìƒì„± ====\n","class SeqDataset(Dataset):\n","    def __init__(self, X, y, menu_ids):\n","        self.X = X     # (N, LOOKBACK, F)\n","        self.y = y     # (N, HORIZON)\n","        self.menu_ids = menu_ids  # (N,)\n","    def __len__(self):\n","        return len(self.X)\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx], self.menu_ids[idx]\n","\n","def build_store_datasets(store_df: pd.DataFrame, exog_cols):\n","    \"\"\"\n","    ë§¤ì¥ ë‚´ ê° ë©”ë‰´ì˜ ì‹œê³„ì—´ë¡œë¶€í„° ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ì–´\n","    (train_dataset, val_dataset, scalers, menu_id_map, feature_dim) ë°˜í™˜\n","    \"\"\"\n","    X_tr_list, y_tr_list, mid_tr_list = [], [], []\n","    X_va_list, y_va_list, mid_va_list = [], [], []\n","\n","    # ë©”ë‰´ ID ë¶€ì—¬\n","    menus = store_df['ë©”ë‰´ëª…'].dropna().unique().tolist()\n","    menu2id = {m:i for i,m in enumerate(sorted(menus))}\n","    n_menus = len(menu2id)\n","\n","    # ë©”ë‰´ë³„ ìŠ¤ì¼€ì¼ëŸ¬(ë§¤ì¶œìˆ˜ëŸ‰ë§Œ ìŠ¤ì¼€ì¼ë§)\n","    scalers = {}\n","\n","    for menu, g in store_df.groupby('ë©”ë‰´ëª…'):\n","        g = g.sort_values('ì˜ì—…ì¼ì').copy()\n","        # í•„ìš”í•œ ì»¬ëŸ¼ ë³´ì •\n","        g['ë§¤ì¶œìˆ˜ëŸ‰'] = g['ë§¤ì¶œìˆ˜ëŸ‰'].fillna(0)\n","\n","        # ê²€ì¦ ê¸°ì¤€ ì¸ë±ìŠ¤(ë§ˆì§€ë§‰ 7ì¼)\n","        # ì‹œí€€ìŠ¤ ìŠ¬ë¼ì´ë”© ì‹œ, íƒ€ê¹ƒ êµ¬ê°„ì˜ ëì´ split_idxë¥¼ ë„˜ìœ¼ë©´ ê²€ì¦ìœ¼ë¡œ ë³´ëƒ„\n","        split_date = g['ì˜ì—…ì¼ì'].max() - pd.Timedelta(days=HORIZON-1)\n","        split_idx = g.index[g['ì˜ì—…ì¼ì'] \u003e= split_date][0]  # ë§ˆì§€ë§‰ 7ì¼ì˜ ì‹œì‘ ìœ„ì¹˜ ì¸ë±ìŠ¤ê°’\n","\n","        train_mask = g.index \u003c split_idx           # ë§ˆì§€ë§‰ 7ì¼ ì‹œì‘ index ì´ì „ë§Œ\n","        scaler = MinMaxScaler().fit(g.loc[train_mask, ['ë§¤ì¶œìˆ˜ëŸ‰']])\n","        g['scaled_sales'] = scaler.transform(g[['ë§¤ì¶œìˆ˜ëŸ‰']])\n","        scalers[menu] = scaler\n","\n","        # ì‹œê³„ì—´ ë°°ì—´ êµ¬ì„±\n","        feats = ['scaled_sales'] + exog_cols\n","        arr = g[feats].fillna(0).values.astype('float32')\n","        mid = np.full((len(g),), menu2id[menu], dtype='int64')\n","\n","        # ì‹œí€€ìŠ¤ ë§Œë“¤ê¸°\n","        # i ... i+LOOKBACK-1 -\u003e ì…ë ¥, i+LOOKBACK ... i+LOOKBACK+HORIZON-1 -\u003e íƒ€ê¹ƒ\n","        for i in range(len(g) - LOOKBACK - HORIZON + 1):\n","            X_seq = arr[i:i+LOOKBACK, :]\n","            y_seq = g['scaled_sales'].values[i+LOOKBACK:i+LOOKBACK+HORIZON]\n","            m_id  = mid[i]\n","\n","            # íƒ€ê¹ƒ êµ¬ê°„ì˜ ë§ˆì§€ë§‰ ì‹œì \n","            target_end_pos = g.index[i+LOOKBACK+HORIZON-1]\n","            if target_end_pos \u003c split_idx:\n","                X_tr_list.append(X_seq)\n","                y_tr_list.append(y_seq)\n","                mid_tr_list.append(m_id)\n","            else:\n","                X_va_list.append(X_seq)\n","                y_va_list.append(y_seq)\n","                mid_va_list.append(m_id)\n","\n","    def to_tensor(xl):\n","        return torch.tensor(np.stack(xl)).float() if len(xl)\u003e0 else torch.empty(0)\n","\n","    X_tr = to_tensor(X_tr_list)\n","    y_tr = to_tensor(y_tr_list)\n","    X_va = to_tensor(X_va_list)\n","    y_va = to_tensor(y_va_list)\n","    mid_tr = torch.tensor(np.array(mid_tr_list), dtype=torch.long) if len(mid_tr_list)\u003e0 else torch.empty(0, dtype=torch.long)\n","    mid_va = torch.tensor(np.array(mid_va_list), dtype=torch.long) if len(mid_va_list)\u003e0 else torch.empty(0, dtype=torch.long)\n","\n","    train_ds = SeqDataset(X_tr, y_tr, mid_tr)\n","    valid_ds = SeqDataset(X_va, y_va, mid_va)\n","\n","    feature_dim = X_tr.shape[-1] if len(X_tr_list)\u003e0 else (X_va.shape[-1] if len(X_va_list)\u003e0 else len(['scaled_sales']+exog_cols))\n","    return train_ds, valid_ds, scalers, menu2id, feature_dim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"u4ozq5dp8PrW"},"outputs":[],"source":["# ==== 3) ëª¨ë¸ ====\n","class StoreLSTM(nn.Module):\n","    def __init__(self, n_menus, feature_dim, embed_dim=8, hidden=128, num_layers=1, dropout=0.1, horizon=7):\n","        super().__init__()\n","        self.embed = nn.Embedding(num_embeddings=max(n_menus,1), embedding_dim=embed_dim)\n","        self.lstm  = nn.LSTM(input_size=feature_dim + embed_dim,\n","                             hidden_size=hidden,\n","                             num_layers=num_layers,\n","                             batch_first=True,\n","                             dropout=dropout if num_layers\u003e1 else 0.0)\n","        self.head  = nn.Sequential(\n","            nn.Linear(hidden, hidden//2),\n","            nn.ReLU(),\n","            nn.Linear(hidden//2, horizon)\n","        )\n","    def forward(self, x, menu_id):\n","        # x: (B, L, F), menu_id: (B,)\n","        emb = self.embed(menu_id)                     # (B, E)\n","        emb = emb.unsqueeze(1).expand(-1, x.size(1), -1)  # (B, L, E)\n","        x_in = torch.cat([x, emb], dim=2)             # (B, L, F+E)\n","        out, (h, c) = self.lstm(x_in)\n","        last = out[:, -1, :]                          # (B, H)\n","        yhat = self.head(last)                        # (B, HORIZON)\n","        return yhat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rcEw3Fdm8QKZ"},"outputs":[],"source":["# ==== 4) í•™ìŠµ ë£¨í”„ ====\n","def train_one_store(store_name, store_df, exog_cols):\n","    print(f\"\\n[Train] ë§¤ì¥: {store_name}\")\n","    train_ds, valid_ds, scalers, menu2id, feature_dim = build_store_datasets(store_df, exog_cols)\n","    n_menus = len(menu2id)\n","\n","    if len(train_ds)==0 or len(valid_ds)==0:\n","        print(\"  â†³ ì‹œí€€ìŠ¤ê°€ ë¶€ì¡±í•˜ì—¬ ìŠ¤í‚µ (train/valid ë¹ˆ ë°ì´í„°)\")\n","        return None\n","\n","    model = StoreLSTM(n_menus=n_menus, feature_dim=feature_dim,\n","                      embed_dim=EMBED_DIM, hidden=HIDDEN, num_layers=NUM_LAYERS,\n","                      dropout=DROPOUT, horizon=HORIZON).to(DEVICE)\n","    crit = SMAPE_MAE_Loss(alpha=0.8)\n","    opt  = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","\n","    best_val = np.inf\n","    best_state = None\n","\n","    for epoch in range(1, EPOCHS+1):\n","        model.train()\n","        tr_losses = []\n","        for Xb, yb, mb in train_loader:\n","            Xb = Xb.to(DEVICE)\n","            yb = yb.to(DEVICE)\n","            mb = mb.to(DEVICE)\n","            opt.zero_grad()\n","            pred = model(Xb, mb)\n","            loss = crit(pred, yb)\n","            loss.backward()\n","            opt.step()\n","            tr_losses.append(loss.item())\n","\n","        # validation\n","        model.eval()\n","        va_losses, all_smape, all_mae = [], [], []\n","        with torch.no_grad():\n","            for Xb, yb, mb in valid_loader:\n","                Xb = Xb.to(DEVICE)\n","                yb = yb.to(DEVICE)\n","                mb = mb.to(DEVICE)\n","                pred = model(Xb, mb)\n","                loss = crit(pred, yb)\n","                va_losses.append(loss.item())\n","                s, m = smape_mae(pred, yb)\n","                all_smape.append(s); all_mae.append(m)\n","\n","        tr_loss = np.mean(tr_losses) if tr_losses else np.nan\n","        va_loss = np.mean(va_losses) if va_losses else np.nan\n","        va_smape = np.mean(all_smape) if all_smape else np.nan\n","        va_mae   = np.mean(all_mae) if all_mae else np.nan\n","        print(f\"  Epoch {epoch:02d} | train {tr_loss:.4f} | valid {va_loss:.4f} | SMAPE {va_smape:.4f} | MAE {va_mae:.4f}\")\n","\n","        if va_loss \u003c best_val:\n","            best_val = va_loss\n","            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n","\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","\n","    return {\n","        \"model\": model,\n","        \"scalers\": scalers,\n","        \"menu2id\": menu2id,\n","        \"exog_cols\": exog_cols,\n","        \"feature_dim\": feature_dim,\n","        \"store\": store_name\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yRsmu8Ie8VK3"},"outputs":[],"source":["import pandas as pd\n","\n","# ==== 5) ì „ì²´ ë§¤ì¥ í•™ìŠµ (ëˆ„ìˆ˜-ì•ˆì „ lag í”¼ì²˜ ì‚¬ìš©) ====\n","def train_lstm_per_store(df1: pd.DataFrame, drop_leaky: bool = True):\n","    # 0) í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬\n","    needed = {'ì˜ì—…ì¼ì','ì˜ì—…ì¥ëª…','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ë©”ë‰´ëª…','ë§¤ì¶œìˆ˜ëŸ‰'}\n","    missing = needed - set(df1.columns)\n","    if missing:\n","        raise ValueError(f\"ë‹¤ìŒ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {missing}\")\n","\n","    df = df1.copy()\n","    df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])\n","\n","    # 1) ê¸°ë³¸ íŒŒìƒ\n","    if 'weekday' not in df.columns:\n","        df['weekday'] = df['ì˜ì—…ì¼ì'].dt.weekday\n","    if 'is_weekend' not in df.columns:\n","        df['is_weekend'] = df['weekday'].isin([5,6]).astype(int)\n","    if 'month' not in df.columns:\n","        df['month'] = df['ì˜ì—…ì¼ì'].dt.month\n","    if 'íœ´ë¬´ì¼' not in df.columns:\n","        df['íœ´ë¬´ì¼'] = 0\n","    if 'custom_offday' not in df.columns:\n","        df['custom_offday'] = 0\n","\n","    # 2) ì›-í•«: í•­ìƒ ê³ ì • ì°¨ì›(weekday=0..6, month=1..12)\n","    df['weekday'] = pd.Categorical(df['weekday'].astype(int), categories=list(range(7)), ordered=True)\n","    df['month']   = pd.Categorical(df['month'].astype(int),   categories=list(range(1,13)), ordered=True)\n","\n","    wd = pd.get_dummies(df['weekday'], prefix='wd', dtype='int8')\n","    m  = pd.get_dummies(df['month'],   prefix='m',  dtype='int8')\n","    # ëˆ„ë½ëœ ë”ë¯¸ê°€ ìˆìœ¼ë©´ ì±„ì›Œ ë„£ê¸°(ì´ë¡ ìƒ ìœ„ Categoricalë¡œ ì´ë¯¸ ëª¨ë‘ ìƒì„±ë¨)\n","    for c in [f'wd_{i}' for i in range(7)]:\n","        if c not in wd.columns: wd[c] = 0\n","    for c in [f'm_{i}' for i in range(1,13)]:\n","        if c not in m.columns: m[c] = 0\n","\n","    wd = wd[[f'wd_{i}' for i in range(7)]]\n","    m  = m[[f'm_{i}'  for i in range(1,13)]]\n","\n","    df = pd.concat([df, wd, m], axis=1)\n","    wd_cols = list(wd.columns)\n","    m_cols  = list(m.columns)\n","\n","    # 3) ì‚¬ìš©í•  ì™¸ìƒ ë³€ìˆ˜(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì±„íƒ)\n","    #    - ëˆ„ìˆ˜-ì•ˆì „ lag1 ê³„ì—´ë§Œ ëª…ì‹œ\n","    lag_safe_candidates = [\n","        # ê³µíœ´ì¼\n","        'is_holiday', 'holiday_prev1', 'holiday_next1',\n","        # ëŸ°ì¹­/ìˆ˜ëª…ì£¼ê¸° (ë‚ ì§œë¡œ ê²°ì •ë˜ëŠ” ì§€í‘œë¼ ëˆ„ìˆ˜ ì•„ë‹˜)\n","        'is_probable_launch',\n","        # ìš´ì˜ í”Œë˜ê·¸\n","        'custom_offday', 'íœ´ë¬´ì¼',\n","        # ë©”ë‰´ ì¤‘ìš”ë„\n","        'is_main_menu',\n","        # ì›” ì§€í‘œ(ëª¨ë‘ lag1ë§Œ ì‚¬ìš©)\n","        'lag1_is_high_season_moy',\n","        'lag1_is_good_month_quantile',\n","        'lag1_is_low_share_month', 'lag1_month_share',\n","        # ì„ íƒ: ì›”í•© ë³€í™”ì˜ lag1 (ìˆì„ ë•Œë§Œ)\n","        'lag1_monthly_sales', 'lag1_monthly_abs_change', 'lag1_monthly_pct_change',\n","    ]\n","\n","    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì±„íƒ + ì›-í•« ë”ë¯¸ ì¶”ê°€\n","    exog_cols = [c for c in lag_safe_candidates if c in df.columns] + wd_cols + m_cols\n","\n","    # 4) ë§¤ì¥ë³„ í•™ìŠµ\n","    trained = {}\n","    for store, g in df.groupby('ì˜ì—…ì¥ëª…', sort=False):\n","        g = g.sort_values('ì˜ì—…ì¼ì')\n","        # ì—¬ê¸°ì„œ ê²°ì¸¡ì´ ìˆë‹¤ë©´ 0ìœ¼ë¡œ ì±„ì›€(ëŒ€ë¶€ë¶„ lag1_*ëŠ” ìƒì„± ì‹œ 0ìœ¼ë¡œ ì±„ì›Œì ¸ ìˆìŒ)\n","        X_cols = [c for c in exog_cols if c in g.columns]\n","        g[X_cols] = g[X_cols].fillna(0)\n","\n","        # train_one_store(store, g, exog_cols) ëŠ” ê¸°ì¡´ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","        result = train_one_store(store, g, X_cols)\n","        if result is not None:\n","            trained[store] = result\n","    return trained\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmHb_ecO82MQ"},"outputs":[],"source":["if 'holiday' in df1.columns:\n","    df1['holiday'] = (\n","        df1['holiday']\n","        .replace({'holiday': 1, 'non-holiday': 0})\n","        .fillna(0)\n","        .astype(int)\n","    )\n","trained_models = train_lstm_per_store(df1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hC5ubRR1vhdf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# ---------------------------------------------\n","# ê³µí†µ ìœ í‹¸: í‚¤/ë‚ ì§œ ë³´ì •\n","# ---------------------------------------------\n","def _ensure_keys_and_date(df, date_col='ì˜ì—…ì¼ì', shop_col='ì˜ì—…ì¥ëª…', menu_col='ë©”ë‰´ëª…', combined_col='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'):\n","    out = df.copy()\n","    out[date_col] = pd.to_datetime(out[date_col])\n","    if (shop_col not in out.columns) or (menu_col not in out.columns):\n","        if combined_col in out.columns:\n","            sp = out[combined_col].astype(str).str.split('_', n=1, expand=True)\n","            out[shop_col] = sp[0]\n","            out[menu_col] = sp[1] if sp.shape[1] \u003e 1 else ''\n","        else:\n","            raise KeyError(\"í›ˆë ¨ DFì— ('ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…') ë˜ëŠ” 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n","    return out\n","\n","# ---------------------------------------------\n","# 1) first_sale_table: ì¶œì‹œì¼(ì²« íŒë§¤ì¼) í‘œ\n","#    -\u003e preprocess_test_df_for_inference(first_sale_table=...)ì— ì‚¬ìš©\n","# ---------------------------------------------\n","def build_first_sale_table(train_df: pd.DataFrame,\n","                           date_col='ì˜ì—…ì¼ì', shop_col='ì˜ì—…ì¥ëª…', menu_col='ë©”ë‰´ëª…',\n","                           sales_col='ë§¤ì¶œìˆ˜ëŸ‰') -\u003e pd.DataFrame:\n","    d = _ensure_keys_and_date(train_df, date_col, shop_col, menu_col)\n","    d = d.sort_values([shop_col, menu_col, date_col])\n","    # ì²« íŒë§¤ì¼(ë§¤ì¶œìˆ˜ëŸ‰ \u003e 0)\n","    first_sale = (d[d[sales_col] \u003e 0]\n","                    .groupby([shop_col, menu_col], as_index=False)[date_col]\n","                    .min()\n","                    .rename(columns={date_col: 'first_sale_date'}))\n","    # ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…ë„ ê°™ì´ ì œê³µ(ë¨¸ì§€ ìœ ì—°ì„±)\n","    first_sale['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = first_sale[shop_col].astype(str) + '_' + first_sale[menu_col].astype(str)\n","    return first_sale\n","\n","# ---------------------------------------------\n","# 2) main_menu_df: ë§¤ì¥ë³„ ì£¼ìš” ë©”ë‰´ ì§‘í•©\n","#    (ê¸°ë³¸ ê·œì¹™: ëˆ„ì  ë§¤ì¶œ ë‚´ë¦¼ì°¨ìˆœì—ì„œ\n","#     - ì—°ì†ë¹„ 0.4 ë¯¸ë§Œ ìµœì´ˆ ì§€ì ê¹Œì§€,\n","#     - ë˜ëŠ” ìµœê³  ë§¤ì¶œì˜ 0.5ë°° ë¯¸ë§Œ ìµœì´ˆ ì§€ì ê¹Œì§€\n","#     ì¤‘ ë” ì—„ê²©í•œ ì»· ì‚¬ìš©)\n","#    -\u003e preprocess_test_df_for_inference(main_menu_df=...)ì— ì‚¬ìš©\n","# ---------------------------------------------\n","def build_main_menu_df(train_df: pd.DataFrame,\n","                       date_col='ì˜ì—…ì¼ì', shop_col='ì˜ì—…ì¥ëª…', menu_col='ë©”ë‰´ëª…',\n","                       sales_col='ë§¤ì¶œìˆ˜ëŸ‰',\n","                       ratio_gap=0.4, half_of_max=0.5) -\u003e pd.DataFrame:\n","    d = _ensure_keys_and_date(train_df, date_col, shop_col, menu_col)\n","    menu_sales = (d.groupby([shop_col, menu_col], as_index=False)[sales_col]\n","                    .sum()\n","                    .rename(columns={sales_col: 'total_menu_sales'}))\n","    rows = []\n","    for store, g in menu_sales.groupby(shop_col, sort=False):\n","        g = g.sort_values('total_menu_sales', ascending=False).reset_index(drop=True)\n","        sales = g['total_menu_sales'].values\n","        if len(sales) == 0:\n","            continue\n","        max_sales = sales[0]\n","        # ì—°ì†ë¹„(ë‹¤ìŒ/ì´ì „)\n","        ratios = sales[1:] / (sales[:-1] + 1e-6)\n","        idx_ratio = np.where(ratios \u003c ratio_gap)[0]\n","        cutoff_ratio = idx_ratio[0] + 1 if len(idx_ratio) \u003e 0 else len(sales)\n","        # ìµœê³  ëŒ€ë¹„ 0.5ë°° ë¯¸ë§Œ\n","        idx_half = np.where(sales \u003c half_of_max * max_sales)[0]\n","        cutoff_half = idx_half[0] if len(idx_half) \u003e 0 else len(sales)\n","        cutoff = min(cutoff_ratio, cutoff_half)\n","        main_menus = set(g.loc[:cutoff-1, menu_col].tolist())\n","        for _, r in g.iterrows():\n","            rows.append({\n","                'ì˜ì—…ì¥ëª…': store,\n","                'ë©”ë‰´ëª…': r[menu_col],\n","                'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': f\"{store}_{r[menu_col]}\",\n","                'is_main_menu': int(r[menu_col] in main_menus)\n","            })\n","    out = pd.DataFrame(rows)\n","    if out.empty:\n","        return pd.DataFrame(columns=['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','is_main_menu'])\n","    out['is_main_menu'] = out['is_main_menu'].astype('int8')\n","    return out\n","\n","# ---------------------------------------------\n","# 3) off_union_mday_df: ë§¤ì¥ë³„ 'ì—°ì† 0ë§¤ì¶œ(\u003e=min_runì¼)' êµ¬ê°„ì˜ (ì›”,ì¼) í•©ì§‘í•©\n","#    -\u003e preprocess_test_df_for_inference(off_union_mday_df=...)ì— ì‚¬ìš©\n","# ---------------------------------------------\n","def build_off_union_mday_df(train_df: pd.DataFrame,\n","                            date_col='ì˜ì—…ì¼ì', store_col='ì˜ì—…ì¥ëª…',\n","                            sales_col='ë§¤ì¶œìˆ˜ëŸ‰', min_run=4) -\u003e pd.DataFrame:\n","    d = _ensure_keys_and_date(train_df, date_col, store_col, 'ë©”ë‰´ëª…')\n","    # ë§¤ì¥Ã—ì¼ì ì´ë§¤ì¶œ\n","    daily = (d.groupby([store_col, date_col], as_index=False)[sales_col]\n","               .sum()\n","               .rename(columns={sales_col: 'store_sales'}))\n","    rows = []\n","    for store, g in daily.groupby(store_col, sort=False):\n","        g = g.sort_values(date_col)\n","        full_idx = pd.date_range(g[date_col].min(), g[date_col].max(), freq='D')\n","        s = (g.set_index(date_col)['store_sales']\n","               .reindex(full_idx)\n","               .fillna(0.0))\n","        z = (s == 0).astype(int)\n","        run_id = (z.diff().fillna(z.iloc[0]) != 0).cumsum()\n","        tmp = (pd.DataFrame({'z': z, 'run_id': run_id})\n","                 .groupby('run_id', group_keys=False)\n","                 .apply(lambda d: pd.Series({\n","                     'flag': d['z'].iloc[0] == 1,\n","                     'start': d.index.min(),\n","                     'end': d.index.max(),\n","                     'len': len(d)\n","                 })))\n","        zero_runs = tmp[(tmp['flag']) \u0026 (tmp['len'] \u003e= min_run)]\n","        if zero_runs.empty:\n","            continue\n","        for _, r in zero_runs.iterrows():\n","            days = pd.date_range(r['start'], r['end'], freq='D')\n","            rows.append(pd.DataFrame({\n","                store_col: store,\n","                'month': days.month,\n","                'day': days.day,\n","                'is_union_offday': 1\n","            }))\n","    if rows:\n","        out = (pd.concat(rows, ignore_index=True)\n","                 .drop_duplicates([store_col, 'month', 'day']))\n","    else:\n","        out = pd.DataFrame(columns=[store_col, 'month', 'day', 'is_union_offday'])\n","    out['is_union_offday'] = out.get('is_union_offday', 0).astype('int8')\n","    return out\n","\n","# ---------------------------------------------\n","# 4) monthly_history: (ì˜ì—…ì¥ëª…, ë©”ë‰´ëª…, ì›”_ê¸°ê°„, monthly_sales)\n","#    -\u003e predict ì „ì²˜ë¦¬ì—ì„œ lag1 ì›” í”¼ì²˜ ê³„ì‚°ì— ì‚¬ìš©\n","# ---------------------------------------------\n","def build_monthly_history(train_df: pd.DataFrame,\n","                          date_col='ì˜ì—…ì¼ì', shop_col='ì˜ì—…ì¥ëª…',\n","                          menu_col='ë©”ë‰´ëª…', sales_col='ë§¤ì¶œìˆ˜ëŸ‰') -\u003e pd.DataFrame:\n","    d = _ensure_keys_and_date(train_df, date_col, shop_col, menu_col)\n","    d['ì›”_ê¸°ê°„'] = d[date_col].dt.to_period('M')\n","    mh = (d.groupby([shop_col, menu_col, 'ì›”_ê¸°ê°„'], as_index=False)[sales_col]\n","            .sum()\n","            .rename(columns={sales_col: 'monthly_sales'}))\n","    return mh\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Cs8Bvz8Evj8h"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n"]},{"name":"stdout","output_type":"stream","text":["         ì˜ì—…ì¥ëª…          ë©”ë‰´ëª… first_sale_date                ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…\n","0  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ      1ì¸ ìˆ˜ì €ì„¸íŠ¸      2023-01-17      ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸\n","1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ    BBQ55(ë‹¨ì²´)      2023-01-03    ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_BBQ55(ë‹¨ì²´)\n","2  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  ëŒ€ì—¬ë£Œ 30,000ì›      2023-01-01  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_ëŒ€ì—¬ë£Œ 30,000ì›\n","is_main_menu\n","0    168\n","1     25\n","Name: count, dtype: int64\n","         ì˜ì—…ì¥ëª…  month  day  is_union_offday\n","0  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ      3    1                1\n","1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ      3    2                1\n","2  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ      3    3                1\n","         ì˜ì—…ì¥ëª…      ë©”ë‰´ëª…     ì›”_ê¸°ê°„  monthly_sales\n","0  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸  2023-01           82.0\n","1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸  2023-02          162.5\n","2  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸  2023-03           39.5\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n","/tmp/ipython-input-248179391.py:105: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda d: pd.Series({\n"]}],"source":["# í›ˆë ¨ DF ì§€ì • (ì˜ˆ: df1 ë˜ëŠ” train_df)\n","train_df = df1  # í›ˆë ¨ êµ¬ê°„ë§Œ í¬í•¨í•´ì•¼ ì•ˆì „í•©ë‹ˆë‹¤.\n","\n","first_sale_table   = build_first_sale_table(train_df)\n","main_menu_df       = build_main_menu_df(train_df, ratio_gap=0.4, half_of_max=0.5)\n","off_union_mday_df  = build_off_union_mday_df(train_df, min_run=4)\n","monthly_history    = build_monthly_history(train_df)\n","\n","# (ì›í•˜ì‹œë©´ ê°„ë‹¨ ì ê²€)\n","print(first_sale_table.head(3))\n","print(main_menu_df['is_main_menu'].value_counts())\n","print(off_union_mday_df.head(3))\n","print(monthly_history.head(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MCyupRL98Zqh"},"outputs":[],"source":["# ==== 6) ì˜ˆì¸¡ í•¨ìˆ˜(ê° ë§¤ì¥ì˜ ëª¨ë“  ë©”ë‰´ì— ëŒ€í•´ 7ì¼ ì˜ˆì¸¡) ====\n","@torch.no_grad()\n","def predict_next_7days(df1: pd.DataFrame, trained_models: dict):\n","    df = df1.copy()\n","    df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])\n","    preds = []\n","\n","    for store, pack in trained_models.items():\n","        model = pack[\"model\"]\n","        model.eval()\n","        scalers = pack[\"scalers\"]\n","        menu2id = pack[\"menu2id\"]\n","        exog_cols = pack[\"exog_cols\"]\n","\n","        sdf = df[df['ì˜ì—…ì¥ëª…']==store].copy()\n","        last_date = sdf['ì˜ì—…ì¼ì'].max()\n","\n","        for menu, g in sdf.groupby('ë©”ë‰´ëª…'):\n","            if menu not in scalers or menu not in menu2id:\n","                continue\n","            g = g.sort_values('ì˜ì—…ì¼ì').copy()\n","            if len(g) \u003c LOOKBACK:\n","                continue\n","\n","            scaler = scalers[menu]\n","            g['scaled_sales'] = scaler.transform(g[['ë§¤ì¶œìˆ˜ëŸ‰']])\n","            feats = ['scaled_sales'] + exog_cols\n","            arr = g[feats].fillna(0).values.astype('float32')\n","\n","            X_seq = torch.tensor(arr[-LOOKBACK:], dtype=torch.float32).unsqueeze(0).to(DEVICE)\n","            m_id  = torch.tensor([menu2id[menu]], dtype=torch.long).to(DEVICE)\n","            yhat_scaled = model(X_seq, m_id).cpu().numpy().ravel()\n","            # ì—­ìŠ¤ì¼€ì¼\n","            yhat = scaler.inverse_transform(yhat_scaled.reshape(-1,1)).ravel()\n","\n","            # ì˜ˆì¸¡ ë‚ ì§œ ìƒì„±\n","            future_days = pd.date_range(last_date + pd.Timedelta(days=1), periods=HORIZON, freq='D')\n","            for d, val in zip(future_days, yhat):\n","                preds.append([store, menu, d, float(max(val, 0.0))])\n","\n","    pred_df = pd.DataFrame(preds, columns=['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì˜ˆì¸¡ì¼ì','ì˜ˆì¸¡ìˆ˜ëŸ‰'])\n","    return pred_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ueH6edQvu_-j"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def preprocess_test_df_for_inference(\n","    test_df: pd.DataFrame,\n","    exog_cols,\n","    *,\n","    # ==== ì›” lag1 í”¼ì²˜ ê³„ì‚°ìš©(í›ˆë ¨ ë°ì´í„°ë¡œë§Œ ë§Œë“  ê²ƒ ê¶Œì¥) ====\n","    monthly_history: pd.DataFrame | None = None,   # ['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì›”_ê¸°ê°„','monthly_sales']\n","\n","    # ==== ì¶œì‹œ/ë©”ë‰´/íœ´ë¬´ ë§¤í•‘(í›ˆë ¨ì—ì„œ ë¯¸ë¦¬ ë§Œë“  í‘œ ì „ë‹¬ ê¶Œì¥) ====\n","    first_sale_table: pd.DataFrame | None = None,  # ['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','first_sale_date']\n","    main_menu_df: pd.DataFrame | None = None,      # ['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','is_main_menu'] ë˜ëŠ” ['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','is_main_menu']\n","    off_union_mday_df: pd.DataFrame | None = None, # ['ì˜ì—…ì¥ëª…','month','day','is_union_offday']  (ì•„ë˜ helper ì°¸ì¡°)\n","\n","    # ==== lag íŒì • íŒŒë¼ë¯¸í„°(í›ˆë ¨ê³¼ ë™ì¼ ìœ ì§€) ====\n","    share_threshold: float = 0.05,\n","    min_total_sales: float | int = 1,\n","    sharp_drop_pct: float = -0.40,\n","    min_abs_drop: float = 20,\n","    min_prev: float = 30,\n","    good_month_quantile: float = 0.75,\n","    rolling_q_window: int = 12,\n","    top_k: int = 3,\n","    bottom_k: int = 3,\n","    min_obs_moy: int = 2,\n","):\n","    \"\"\"\n","    í…ŒìŠ¤íŠ¸ DF ì „ì²˜ë¦¬:\n","      - í‚¤/ë‚ ì§œ íŒŒìƒ + ê³µíœ´ì¼/ì „í›„ì¼\n","      - wd_0..6, m_1..12 ê³ ì • ì›í•«\n","      - í›ˆë ¨ history ê¸°ë°˜ lag1 ì›” í”¼ì²˜\n","      - âœ¨ ì¶œì‹œ ì´í›„ í”Œë˜ê·¸(is_probable_launch=1) ì ìš©(= post-launch)\n","      - âœ¨ ë§¤ì¥ë³„ 'ì—°ì† 0ë§¤ì¶œ(\u003e=4ì¼)' êµ¬ê°„ì˜ ì›”-ì¼ í•©ì§‘í•©ìœ¼ë¡œ íœ´ë¬´ì¼=1\n","      - âœ¨ íœ´ë¬´ì¼==0ì—ì„œë§Œ custom_offday ê·œì¹™ ì ìš©\n","      - âœ¨ í›ˆë ¨ì—ì„œ ì‚°ì •í•œ main_menu ì§‘í•© ê·¸ëŒ€ë¡œ ì ìš©\n","      - exog_cols ì¤‘ ëˆ„ë½ì€ 0ìœ¼ë¡œ ë³´ì™„\n","    \"\"\"\n","    df = test_df.copy()\n","\n","    # â”€â”€ 0) í‚¤/ë‚ ì§œ íŒŒìƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    if ('ì˜ì—…ì¥ëª…' not in df.columns) or ('ë©”ë‰´ëª…' not in df.columns):\n","        if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' not in df.columns:\n","            raise KeyError(\"í…ŒìŠ¤íŠ¸ DFì— 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' ë˜ëŠ” ('ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…')ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n","        sp = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].astype(str).str.split('_', n=1, expand=True)\n","        df['ì˜ì—…ì¥ëª…'] = sp[0]; df['ë©”ë‰´ëª…'] = sp[1] if sp.shape[1] \u003e 1 else ''\n","\n","    df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])\n","    df['weekday']  = df['ì˜ì—…ì¼ì'].dt.weekday\n","    df['month']    = df['ì˜ì—…ì¼ì'].dt.month\n","    df['day']      = df['ì˜ì—…ì¼ì'].dt.day\n","    df['ì›”_ê¸°ê°„']    = df['ì˜ì—…ì¼ì'].dt.to_period('M')\n","\n","    # â”€â”€ 1) ê³µíœ´ì¼/ì „í›„ì¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    try:\n","        import holidays as pyholidays\n","        years = list(range(df['ì˜ì—…ì¼ì'].dt.year.min(), df['ì˜ì—…ì¼ì'].dt.year.max() + 1))\n","        kr = pyholidays.KR(years=years)\n","        cal = pd.DataFrame({'ì˜ì—…ì¼ì': pd.date_range(df['ì˜ì—…ì¼ì'].min(), df['ì˜ì—…ì¼ì'].max(), freq='D')})\n","        cal['is_holiday'] = cal['ì˜ì—…ì¼ì'].dt.date.map(lambda d: 1 if d in kr else 0).astype('int8')\n","    except Exception:\n","        cal = pd.DataFrame({'ì˜ì—…ì¼ì': pd.date_range(df['ì˜ì—…ì¼ì'].min(), df['ì˜ì—…ì¼ì'].max(), freq='D')})\n","        fixed_days = {(1,1),(3,1),(5,5),(6,6),(8,15),(10,3),(10,9),(12,25)}\n","        cal['is_holiday'] = cal['ì˜ì—…ì¼ì'].map(lambda d: 1 if (d.month, d.day) in fixed_days else 0).astype('int8')\n","\n","    cal = cal.sort_values('ì˜ì—…ì¼ì')\n","    cal['holiday_prev1'] = cal['is_holiday'].shift(1).fillna(0).astype('int8')\n","    cal['holiday_next1'] = cal['is_holiday'].shift(-1).fillna(0).astype('int8')\n","    df = df.merge(cal, on='ì˜ì—…ì¼ì', how='left')\n","\n","    # â”€â”€ 2) ìš”ì¼/ì›” ì›í•«(ê³ ì • ì°¨ì›) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    df['weekday'] = pd.Categorical(df['weekday'].astype(int), categories=list(range(7)), ordered=True)\n","    df['month']   = pd.Categorical(df['month'].astype(int),   categories=list(range(1,13)), ordered=True)\n","    wd = pd.get_dummies(df['weekday'], prefix='wd', dtype='int8')[[f'wd_{i}' for i in range(7)]]\n","    m  = pd.get_dummies(df['month'],   prefix='m',  dtype='int8')[[f'm_{i}' for i in range(1,13)]]\n","    df = pd.concat([df, wd, m], axis=1)\n","\n","    # â”€â”€ 3) lag1 ì›” í”¼ì²˜(í›ˆë ¨ historyë¡œ ê³„ì‚°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    lag_cols = [\n","        'lag1_month_share','lag1_is_low_share_month',\n","        'lag1_monthly_sales','lag1_monthly_abs_change','lag1_monthly_pct_change',\n","        'lag1_is_good_month_quantile',\n","        'lag1_is_high_season_moy',\n","    ]\n","    for c in lag_cols:\n","        if c not in df.columns:\n","            df[c] = 0  # ê¸°ë³¸ê°’\n","\n","    if monthly_history is not None and len(monthly_history):\n","        mh = monthly_history.copy()\n","        if 'ì›”_ê¸°ê°„' not in mh.columns:\n","            raise KeyError(\"monthly_historyì—ëŠ” 'ì›”_ê¸°ê°„'(period[M]) ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n","        if not isinstance(mh['ì›”_ê¸°ê°„'].dtype, pd.PeriodDtype):\n","            mh['ì›”_ê¸°ê°„'] = pd.PeriodIndex(mh['ì›”_ê¸°ê°„'], freq='M')\n","        for c in ['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì›”_ê¸°ê°„','monthly_sales']:\n","            if c not in mh.columns:\n","                raise KeyError(f\"monthly_historyì— '{c}' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n","\n","        feats_rows = []\n","        test_months = df[['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì›”_ê¸°ê°„']].drop_duplicates()\n","\n","        for (store, menu), g_hist in mh.groupby(['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…'], sort=False):\n","            s = g_hist.sort_values('ì›”_ê¸°ê°„').set_index('ì›”_ê¸°ê°„')['monthly_sales'].astype(float)\n","            if s.empty:\n","                continue\n","            cum = s.cumsum()\n","            s_shift = s.shift(1)\n","\n","            # MOY ëˆ„ì  í‰ê·  ì¤€ë¹„\n","            month_idx = pd.Index([p.month for p in s.index], name='m')\n","            cum_sum = {mo: (s.where(month_idx==mo)).cumsum().shift(1) for mo in range(1,13)}\n","            cum_cnt = {mo: (s.where(month_idx==mo).notna().astype(int)).cumsum().shift(1) for mo in range(1,13)}\n","\n","            tms = test_months[(test_months['ì˜ì—…ì¥ëª…']==store) \u0026 (test_months['ë©”ë‰´ëª…']==menu)]['ì›”_ê¸°ê°„']\n","            for tm in tms:\n","                prev, prev2 = tm-1, tm-2\n","                m_prev  = float(s.get(prev, 0.0))\n","                m_prev2 = s.get(prev2, np.nan)\n","                cum_prev = float(cum.get(prev, 0.0))\n","\n","                # ì ìœ ìœ¨/low-share\n","                l_share = (m_prev / cum_prev) if cum_prev \u003e 0 else 0.0\n","                l_low   = int((cum_prev \u003e= float(min_total_sales)) and (l_share \u003c= float(share_threshold)))\n","\n","                # ì „ì›” ë³€í™”\n","                if pd.isna(m_prev2) or (float(m_prev2) == 0.0):\n","                    l_abs = float(m_prev - (0.0 if pd.isna(m_prev2) else float(m_prev2)))\n","                    l_pct = 0.0\n","                else:\n","                    l_abs = float(m_prev - float(m_prev2))\n","                    l_pct = float(l_abs / float(m_prev2))\n","\n","                # ê¸‰ë½\n","                l_drop = int((not pd.isna(m_prev2)) and (float(m_prev2) \u003e= float(min_prev)) and\n","                             (l_pct \u003c= float(sharp_drop_pct)) and (abs(l_abs) \u003e= float(min_abs_drop)))\n","\n","                # ì¢‹ì€ ë‹¬ ë¶„ìœ„\n","                past = s_shift.loc[:prev].dropna()\n","                if len(past) \u003e= max(3, rolling_q_window//2):\n","                    thr = past.tail(rolling_q_window).quantile(good_month_quantile)\n","                    l_good = int(m_prev \u003e= float(thr))\n","                else:\n","                    l_good = 0\n","\n","                # MOY ìƒ/í•˜ìœ„\n","                mo_prev = prev.month\n","                vals, idxs = [], []\n","                for mo in range(1,13):\n","                    cs, cc = cum_sum[mo], cum_cnt[mo]\n","                    if prev in cs.index:\n","                        ssum, scnt = cs.loc[prev], cc.loc[prev]\n","                        if pd.notna(scnt) and scnt \u003e= min_obs_moy and pd.notna(ssum):\n","                            vals.append(float(ssum/scnt)); idxs.append(mo-1)\n","                if len(vals) \u003e 0:\n","                    order = np.argsort(vals)\n","                    top_idx = np.array(idxs)[order[-min(top_k,len(vals)):]] if top_k\u003e0 else np.array([],dtype=int)\n","                    low_idx = np.array(idxs)[order[:min(bottom_k,len(vals))]] if bottom_k\u003e0 else np.array([],dtype=int)\n","                    l_high = int((mo_prev-1) in top_idx)\n","                    l_lowm = int((mo_prev-1) in low_idx)\n","                else:\n","                    l_high = 0; l_lowm = 0\n","\n","                feats_rows.append({\n","                    'ì˜ì—…ì¥ëª…': store, 'ë©”ë‰´ëª…': menu, 'ì›”_ê¸°ê°„': tm,\n","                    'lag1_month_share': np.float32(l_share),\n","                    'lag1_is_low_share_month': np.int8(l_low),\n","                    'lag1_monthly_sales': np.float32(m_prev),\n","                    'lag1_monthly_abs_change': np.float32(l_abs),\n","                    'lag1_monthly_pct_change': np.float32(l_pct),\n","                    'lag1_is_sharp_drop_month': np.int8(l_drop),\n","                    'lag1_is_good_month_quantile': np.int8(l_good),\n","                    'lag1_is_high_season_moy': np.int8(l_high),\n","                    'lag1_is_low_season_moy':  np.int8(l_lowm),\n","                })\n","\n","        if feats_rows:\n","            lag_df = pd.DataFrame(feats_rows)\n","            df = df.merge(lag_df, on=['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì›”_ê¸°ê°„'], how='left', suffixes=('', '_calc'))\n","            for col, dtype, fill in [\n","                ('lag1_month_share','float32',0.0),\n","                ('lag1_is_low_share_month','int8',0),\n","                ('lag1_monthly_sales','float32',0.0),\n","                ('lag1_monthly_abs_change','float32',0.0),\n","                ('lag1_monthly_pct_change','float32',0.0),\n","                ('lag1_is_good_month_quantile','int8',0),\n","                ('lag1_is_high_season_moy','int8',0),\n","            ]:\n","                src = col + '_calc'\n","                if src in df.columns:\n","                    df[col] = df[src]; df.drop(columns=[src], inplace=True)\n","                df[col] = df[col].fillna(fill).astype(dtype)\n","\n","    # â”€â”€ 4) ì¶œì‹œ ì´í›„ í”Œë˜ê·¸ = is_probable_launch (ìš”ì²­ëŒ€ë¡œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    #     ì •ì˜: í•´ë‹¹ (ì˜ì—…ì¥ëª…,ë©”ë‰´ëª…)ì˜ first_sale_date ì´í›„ì¸ ë‚ ì§œë©´ 1, ì•„ë‹ˆë©´ 0\n","    if ('is_probable_launch' in exog_cols):\n","        if (first_sale_table is not None) and len(first_sale_table):\n","            fst = first_sale_table.copy()\n","            # í‚¤ ì •ë ¬\n","            if 'ì˜ì—…ì¥ëª…' not in fst.columns or 'ë©”ë‰´ëª…' not in fst.columns:\n","                if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' in fst.columns:\n","                    sp = fst['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].astype(str).str.split('_', n=1, expand=True)\n","                    fst['ì˜ì—…ì¥ëª…'] = sp[0]; fst['ë©”ë‰´ëª…'] = sp[1] if sp.shape[1]\u003e1 else ''\n","                else:\n","                    raise KeyError(\"first_sale_tableì—ëŠ” ('ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…') ë˜ëŠ” 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n","            if 'first_sale_date' not in fst.columns:\n","                raise KeyError(\"first_sale_tableì—ëŠ” 'first_sale_date'ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n","            fst['first_sale_date'] = pd.to_datetime(fst['first_sale_date'])\n","            df = df.merge(fst[['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','first_sale_date']], on=['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…'], how='left')\n","            df['is_probable_launch'] = (df['ì˜ì—…ì¼ì'] \u003e= df['first_sale_date']).astype('int8').fillna(0)\n","            df.drop(columns=['first_sale_date'], inplace=True)\n","        else:\n","            # ì •ë³´ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ\n","            if 'is_probable_launch' not in df.columns:\n","                df['is_probable_launch'] = 0\n","\n","    # â”€â”€ 5) íœ´ë¬´ì¼: ë§¤ì¥ë³„ 'ì—°ì† 0ë§¤ì¶œ(\u003e=4ì¼)' êµ¬ê°„ì˜ ì›”-ì¼ í•©ì§‘í•©ìœ¼ë¡œ ì„¤ì • â”€\n","    #     (í›ˆë ¨ì—ì„œ ë§Œë“  off_union_mday_df ì „ë‹¬ ê¶Œì¥)\n","    if ('íœ´ë¬´ì¼' in exog_cols):\n","        # ì‹œì‘ê°’(ì—†ìœ¼ë©´ 0)\n","        if 'íœ´ë¬´ì¼' not in df.columns:\n","            df['íœ´ë¬´ì¼'] = 0\n","        if (off_union_mday_df is not None) and len(off_union_mday_df):\n","            offu = off_union_mday_df.copy()\n","            # í•„ìš”í•œ ì»¬ëŸ¼ ì²´í¬\n","            for c in ['ì˜ì—…ì¥ëª…','month','day','is_union_offday']:\n","                if c not in offu.columns:\n","                    raise KeyError(\"off_union_mday_dfì—ëŠ” ['ì˜ì—…ì¥ëª…','month','day','is_union_offday']ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n","            tmp = df[['ì˜ì—…ì¥ëª…','month','day']].merge(\n","                offu, on=['ì˜ì—…ì¥ëª…','month','day'], how='left'\n","            )['is_union_offday'].fillna(0).astype('int8')\n","            # ê¸°ì¡´ íœ´ë¬´ì¼ê³¼ OR\n","            df['íœ´ë¬´ì¼'] = ((df['íœ´ë¬´ì¼'].fillna(0).astype(int) \u003e 0) | (tmp == 1)).astype('int8')\n","\n","    # â”€â”€ 6) custom_offday: íœ´ë¬´ì¼==0ì—ì„œë§Œ ê·œì¹™ ì ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    if ('custom_offday' in exog_cols):\n","        if 'custom_offday' not in df.columns:\n","            df['custom_offday'] = 0\n","        # ì›”, ìš”ì¼ ì¤€ë¹„ (ìˆ«ìí˜•)\n","        mo = df['month'].astype(int)\n","        wd = df['weekday'].astype(int)\n","        is_op = (df['íœ´ë¬´ì¼'].fillna(0).astype(int) == 0).values  # ì˜ì—… ì¤‘ì¼ ë•Œë§Œ\n","        # í¬ë ˆìŠ¤íŠ¸ë¦¿: 4,5,9,10,11 \u0026 ì›”ìš”ì¼\n","        mask = (\n","            (df['ì˜ì—…ì¥ëª…'] == 'í¬ë ˆìŠ¤íŠ¸ë¦¿') \u0026\n","            (mo.isin([4,5,9,10,11])) \u0026\n","            (wd == 0) \u0026 is_op\n","        )\n","        df.loc[mask, 'custom_offday'] = 1\n","        # ì—°íšŒì¥: ì¼ìš”ì¼\n","        mask = ((df['ì˜ì—…ì¥ëª…'] == 'ì—°íšŒì¥') \u0026 (wd == 6) \u0026 is_op)\n","        df.loc[mask, 'custom_offday'] = 1\n","        # í™”ë‹´ìˆ²ì£¼ë§‰/í™”ë‹´ìˆ²ì¹´í˜/ë¼ê·¸ë¡œíƒ€/ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ: ì›”ìš”ì¼\n","        mask = (\n","            df['ì˜ì—…ì¥ëª…'].isin(['í™”ë‹´ìˆ²ì£¼ë§‰','í™”ë‹´ìˆ²ì¹´í˜','ë¼ê·¸ë¡œíƒ€','ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ']) \u0026\n","            (wd == 0) \u0026 is_op\n","        )\n","        df.loc[mask, 'custom_offday'] = 1\n","        df['custom_offday'] = df['custom_offday'].astype('int8')\n","\n","    # â”€â”€ 7) main_menu: í›ˆë ¨ì—ì„œì˜ ì§‘í•© ê·¸ëŒ€ë¡œ ì ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    if ('is_main_menu' in exog_cols):\n","        if (main_menu_df is not None) and len(main_menu_df):\n","            mm = main_menu_df.copy()\n","            if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' in mm.columns and (('ì˜ì—…ì¥ëª…' not in mm.columns) or ('ë©”ë‰´ëª…' not in mm.columns)):\n","                # ê·¸ëŒ€ë¡œ merge\n","                if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' not in df.columns:\n","                    df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = df['ì˜ì—…ì¥ëª…'].astype(str) + '_' + df['ë©”ë‰´ëª…'].astype(str)\n","                df = df.merge(mm[['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','is_main_menu']], on='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', how='left')\n","            else:\n","                df = df.merge(mm[['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','is_main_menu']], on=['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…'], how='left')\n","            df['is_main_menu'] = df['is_main_menu'].fillna(0).astype('int8')\n","        else:\n","            if 'is_main_menu' not in df.columns:\n","                df['is_main_menu'] = 0\n","\n","    # â”€â”€ 8) ê¸°íƒ€ ì™¸ìƒë³€ìˆ˜ ê¸°ë³¸ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    default_zero = [\n","        'is_zero_sales_period', 'is_coldstart_60d'  # í•„ìš”ì‹œ 0\n","    ]\n","    for c in default_zero:\n","        if c in exog_cols and c not in df.columns:\n","            df[c] = 0\n","\n","    # object â†’ ìˆ«ì\n","    for c in exog_cols:\n","        if c in df.columns and df[c].dtype == 'object':\n","            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n","\n","    # ìµœì¢…: exog_cols ëª¨ë‘ ë³´ìœ \n","    for c in exog_cols:\n","        if c not in df.columns:\n","            df[c] = 0\n","\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rNdTwVU7v9HV"},"outputs":[],"source":["@torch.no_grad()\n","def predict_next_7days_from_test(\n","    test_df: pd.DataFrame,\n","    trained_models: dict,\n","    *,\n","    # âœ… í›ˆë ¨ êµ¬ê°„ì—ì„œ ë§Œë“  (ì˜ì—…ì¥ëª…, ë©”ë‰´ëª…, ì›”_ê¸°ê°„, monthly_sales) í…Œì´ë¸” ê¶Œì¥\n","    monthly_history: pd.DataFrame | None = None,\n","    # âœ… ì „ì²˜ë¦¬ì— ì“°ì¼ ë§¤í•‘(í›ˆë ¨ ë°ì´í„°ë¡œ ë§Œë“  í‘œ ì „ë‹¬)\n","    first_sale_table: pd.DataFrame | None = None,   # ['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','first_sale_date'] ë˜ëŠ” 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'\n","    main_menu_df: pd.DataFrame | None = None,       # ['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','is_main_menu'] ë˜ëŠ” 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'\n","    off_union_mday_df: pd.DataFrame | None = None,  # ['ì˜ì—…ì¥ëª…','month','day','is_union_offday']\n","\n","    # ì•„ë˜ íŒŒë¼ë¯¸í„°ëŠ” í›ˆë ¨ ë•Œì™€ ë™ì¼ê°’ ìœ ì§€ ê¶Œì¥ (lag1 ì›” í”¼ì²˜ ê³„ì‚°ìš©)\n","    share_threshold: float = 0.05,\n","    min_total_sales: float | int = 1,\n","    sharp_drop_pct: float = -0.40,\n","    min_abs_drop: float = 20,\n","    min_prev: float = 30,\n","    good_month_quantile: float = 0.75,\n","    rolling_q_window: int = 12,\n","    top_k: int = 3,\n","    bottom_k: int = 3,\n","):\n","    \"\"\"\n","    trained_models: train_lstm_per_store(df1) ê²°ê³¼(dict: store -\u003e pack)\n","      - pack: {'model','menu2id','scalers','exog_cols', ...}\n","    test_df: ì´ íŒŒì¼(ìµœê·¼ 28ì¼ì¹˜)ë§Œ ì‚¬ìš©í•´ 7ì¼ ì˜ˆì¸¡\n","    monthly_history: (ì„ íƒ) í›ˆë ¨ êµ¬ê°„ì˜ ì›”ë³„ ì§‘ê³„. ìˆìœ¼ë©´ lag1 í”¼ì²˜ê°€ ì •í™•í•´ì§‘ë‹ˆë‹¤.\n","    first_sale_table: ì¶œì‹œì¼ í…Œì´ë¸”(ì¶œì‹œ ì´í›„ is_probable_launch=1 ì ìš©)\n","    main_menu_df:     í›ˆë ¨ì—ì„œ ì‚°ì •í•œ ì£¼ìš”ë©”ë‰´ ì§‘í•© ê·¸ëŒ€ë¡œ ë°˜ì˜\n","    off_union_mday_df:í›ˆë ¨ì—ì„œ ì°¾ì€ 'ì—°ì† 0ë§¤ì¶œ(\u003e=4ì¼)' êµ¬ê°„ì˜ (ì›”,ì¼) í•©ì§‘í•©ìœ¼ë¡œ íœ´ë¬´ì¼=1 ì ìš©\n","    ë°˜í™˜: ['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì˜ˆì¸¡ì¼ì','ì˜ˆì¸¡ìˆ˜ëŸ‰']\n","    \"\"\"\n","    # 0) ëª¨ë“  ë§¤ì¥ì—ì„œ ì‚¬ìš©ëœ exogë“¤ì˜ í•©ì§‘í•© (ğŸ”’ëˆ„ìˆ˜ ê°€ëŠ¥ ì›ì²œì€ ì°¨ë‹¨)\n","    leaky_ban = {\n","        'is_high_season_moy','is_low_season_moy',\n","        'is_good_month_quantile','is_sharp_drop_month',\n","        'is_low_share_month','month_share',\n","        'monthly_sales','monthly_abs_change','monthly_pct_change',\n","    }\n","    exog_cols_all = sorted({\n","        c for pack in trained_models.values() for c in pack['exog_cols']\n","        if c not in leaky_ban\n","    })\n","\n","    # 1) í…ŒìŠ¤íŠ¸ ì „ì²˜ë¦¬(âœ… í•™ìŠµê³¼ ë™ì¼ ê·œì¹™, lag1 í”¼ì²˜ ê³„ì‚° + ì¶œì‹œ/íœ´ë¬´/ì»¤ìŠ¤í…€ì˜¤í”„/ì£¼ìš”ë©”ë‰´ ë°˜ì˜)\n","    df = preprocess_test_df_for_inference(\n","        test_df,\n","        exog_cols=exog_cols_all,\n","        monthly_history=monthly_history,\n","        first_sale_table=first_sale_table,\n","        main_menu_df=main_menu_df,\n","        off_union_mday_df=off_union_mday_df,\n","        # lag1 ê³„ì‚° í•˜ì´í¼íŒŒë¼ë¯¸í„°(í›ˆë ¨ê°’ê³¼ ì¼ì¹˜ ê¶Œì¥)\n","        share_threshold=share_threshold,\n","        min_total_sales=min_total_sales,\n","        sharp_drop_pct=sharp_drop_pct,\n","        min_abs_drop=min_abs_drop,\n","        min_prev=min_prev,\n","        good_month_quantile=good_month_quantile,\n","        rolling_q_window=rolling_q_window,\n","        top_k=top_k,\n","        bottom_k=bottom_k,\n","    )\n","\n","    preds = []\n","    for store, pack in trained_models.items():\n","        sdf = df[df['ì˜ì—…ì¥ëª…'] == store].copy()\n","        if sdf.empty:\n","            continue\n","\n","        last_date = sdf['ì˜ì—…ì¼ì'].max()\n","        menu2id  = pack['menu2id']\n","        scalers  = pack['scalers']\n","        # ë§¤ì¥ë³„ exogë„ ì•ˆì „í•˜ê²Œ ëˆ„ìˆ˜ ì»¬ëŸ¼ ì œê±° + ì‹¤ì œ ë³´ìœ  ì»¬ëŸ¼ë§Œ ì‚¬ìš©\n","        exog_cols = [c for c in pack['exog_cols'] if (c in sdf.columns) and (c not in leaky_ban)]\n","        model    = pack['model']\n","        model.eval()\n","\n","        for menu, g in sdf.groupby('ë©”ë‰´ëª…'):\n","            if (menu not in menu2id) or (menu not in scalers):\n","                # í•™ìŠµ ì œì™¸ ë©”ë‰´ì¼ ìˆ˜ ìˆìŒ\n","                continue\n","            g = g.sort_values('ì˜ì—…ì¼ì').copy()\n","            if len(g) \u003c LOOKBACK:\n","                continue\n","\n","            # ìŠ¤ì¼€ì¼ \u0026 í”¼ì²˜ êµ¬ì„±\n","            scaler = scalers[menu]\n","            g['scaled_sales'] = scaler.transform(g[['ë§¤ì¶œìˆ˜ëŸ‰']])\n","            feats = ['scaled_sales'] + exog_cols\n","            arr = g[feats].fillna(0).values.astype('float32')\n","\n","            X    = torch.tensor(arr[-LOOKBACK:], dtype=torch.float32).unsqueeze(0).to(DEVICE)\n","            m_id = torch.tensor([menu2id[menu]], dtype=torch.long).to(DEVICE)\n","\n","            yhat_scaled = model(X, m_id).cpu().numpy().ravel()\n","            yhat = scaler.inverse_transform(yhat_scaled.reshape(-1, 1)).ravel()\n","\n","            future_days = pd.date_range(last_date + pd.Timedelta(days=1), periods=HORIZON, freq='D')\n","            for d, val in zip(future_days, yhat):\n","                preds.append([store, menu, d, float(max(val, 0.0))])\n","\n","    pred_df = pd.DataFrame(preds, columns=['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì˜ˆì¸¡ì¼ì','ì˜ˆì¸¡ìˆ˜ëŸ‰'])\n","    return pred_df\n"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14614,"status":"ok","timestamp":1755251571557,"user":{"displayName":"Sumin Kim","userId":"08128588576306778232"},"user_tz":-540},"id":"M4dc2BaO_xPs","outputId":"e74d6943-069c-4627-abda-7a2e98eb14a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["         ì˜ì—…ì¥ëª…      ë©”ë‰´ëª…       ì˜ˆì¸¡ì¼ì      ì˜ˆì¸¡ìˆ˜ëŸ‰       source\n","0  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2024-07-14  5.726124  TEST_00.csv\n","1  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2024-07-15  4.982970  TEST_00.csv\n","2  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2024-07-16  2.672959  TEST_00.csv\n","3  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2024-07-17  3.315212  TEST_00.csv\n","4  ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  1ì¸ ìˆ˜ì €ì„¸íŠ¸ 2024-07-18  3.541336  TEST_00.csv\n"]}],"source":["all_preds = []\n","for path in sorted(glob.glob('/content/drive/MyDrive/Colab Notebooks/LGaimers/á„’á…¢á„á…¥á„á…©á†«/test/TEST_*.csv')):\n","    test_df = pd.read_csv(path)\n","    pred_df = predict_next_7days_from_test(\n","        test_df,\n","        trained_models,\n","        monthly_history=monthly_history,     # ê¶Œì¥\n","        first_sale_table=first_sale_table,   # âœ… ì¶”ê°€\n","        main_menu_df=main_menu_df,           # âœ… ì¶”ê°€\n","        off_union_mday_df=off_union_mday_df, # âœ… ì¶”ê°€\n","        share_threshold=0.05, min_total_sales=1,\n","        sharp_drop_pct=-0.40, min_abs_drop=20, min_prev=30,\n","        good_month_quantile=0.75, rolling_q_window=12,\n","        top_k=3, bottom_k=3\n","    )\n","    pred_df['source'] = os.path.basename(path)  # (ì„ íƒ) ì¶”ì ìš©\n","    all_preds.append(pred_df)\n","\n","full_pred_df = pd.concat(all_preds, ignore_index=True)\n","print(full_pred_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"g7RljmALHGbi"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","def convert_predictions_to_submission_flexible(\n","    pred_df: pd.DataFrame,\n","    template_csv_path: str,\n","    output_csv_path: str,\n","    round_to_int: bool = True,\n","    horizon: int = 7\n",") -\u003e pd.DataFrame:\n","    sub = pd.read_csv(template_csv_path, dtype=str).copy()\n","    if 'ì˜ì—…ì¼ì' not in sub.columns:\n","        raise ValueError(\"í…œí”Œë¦¿ì— 'ì˜ì—…ì¼ì' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.\")\n","    target_cols = [c for c in sub.columns if c != 'ì˜ì—…ì¼ì']\n","    id_values = sub['ì˜ì—…ì¼ì'].astype(str)\n","    id_mode = id_values.str.match(r'^TEST_\\d+\\+\\d+ì¼$').all()\n","\n","    pred = pred_df.copy()\n","    if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' not in pred.columns:\n","        pred['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = pred['ì˜ì—…ì¥ëª…'].astype(str) + '_' + pred['ë©”ë‰´ëª…'].astype(str)\n","\n","    if id_mode:\n","        if 'source' not in pred.columns:\n","            raise ValueError(\"ID ëª¨ë“œ ë³€í™˜ì—ëŠ” pred_dfì— 'source' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n","\n","        def _prefix_from_source(s):\n","            m = re.search(r'(TEST_\\d+)', str(s))\n","            return m.group(1) if m else None\n","\n","        pred['prefix'] = pred['source'].map(_prefix_from_source)\n","        pred = pred.sort_values(['prefix','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ˆì¸¡ì¼ì'])\n","        pred['ì¼ì˜¤í”„ì…‹'] = pred.groupby(['prefix','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']).cumcount() + 1\n","        pred = pred[pred['ì¼ì˜¤í”„ì…‹'].between(1, horizon)]\n","        pred['row_id'] = pred['prefix'] + '+' + pred['ì¼ì˜¤í”„ì…‹'].astype(str) + 'ì¼'\n","\n","        pivot = pred.pivot_table(index='row_id',\n","                                 columns='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…',\n","                                 values='ì˜ˆì¸¡ìˆ˜ëŸ‰',\n","                                 aggfunc='sum').reindex(columns=target_cols)\n","\n","        # ğŸ”§ í•µì‹¬: í‚¤ë¡œ ë³‘í•©\n","        out = sub[['ì˜ì—…ì¼ì']].merge(pivot, left_on='ì˜ì—…ì¼ì', right_index=True, how='left')\n","\n","    else:\n","        # ë‚ ì§œ í…œí”Œë¦¿\n","        sub['_dt'] = pd.to_datetime(sub['ì˜ì—…ì¼ì'], errors='coerce', format='%Y-%m-%d')\n","        if sub['_dt'].isna().any():\n","            sub['_dt'] = pd.to_datetime(sub['ì˜ì—…ì¼ì'], errors='coerce')\n","        if sub['_dt'].isna().any():\n","            bad = sub['ì˜ì—…ì¼ì'][sub['_dt'].isna()].unique()[:5]\n","            raise ValueError(f\"í…œí”Œë¦¿ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨. ì˜ˆ: {bad}\")\n","\n","        pred['ì˜ˆì¸¡ì¼ì'] = pd.to_datetime(pred['ì˜ˆì¸¡ì¼ì'])\n","        pivot = pred.pivot_table(index='ì˜ˆì¸¡ì¼ì',\n","                                 columns='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…',\n","                                 values='ì˜ˆì¸¡ìˆ˜ëŸ‰',\n","                                 aggfunc='sum').reindex(columns=target_cols)\n","\n","        # ğŸ”§ í•µì‹¬: ë‚ ì§œ í‚¤ë¡œ ë³‘í•©\n","        out = sub[['ì˜ì—…ì¼ì','_dt']].merge(pivot, left_on='_dt', right_index=True, how='left').drop(columns=['_dt'])\n","\n","    # í›„ì²˜ë¦¬\n","    for c in target_cols:\n","        out[c] = pd.to_numeric(out[c], errors='coerce').fillna(0)\n","    out[target_cols] = out[target_cols].clip(lower=0)\n","    if round_to_int:\n","        out[target_cols] = out[target_cols].round().astype(int)\n","\n","    # ìµœì¢… ì—´ ìˆœì„œ ì •ë¦¬\n","    out = out[['ì˜ì—…ì¼ì'] + target_cols]\n","    out.to_csv(output_csv_path, index=False)\n","    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_csv_path}\")\n","    return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvESGt-AAbXb"},"outputs":[],"source":["converted = convert_predictions_to_submission_flexible(\n","    pred_df=full_pred_df,\n","    template_csv_path='/content/drive/MyDrive/Colab Notebooks/LGaimers/á„’á…¢á„á…¥á„á…©á†«/sample_submission.csv',\n","    output_csv_path='/content/drive/MyDrive/Colab Notebooks/LGaimers/á„’á…¢á„á…¥á„á…©á†«/train/submission_1.csv',\n","    round_to_int=True\n",")\n","converted.to_csv(\n","    '/content/drive/MyDrive/Colab Notebooks/LGaimers/í•´ì»¤í†¤/train/submission_1_utf8sig.csv',\n","    index=False, encoding='utf-8-sig'\n",")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}